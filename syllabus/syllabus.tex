\documentclass[12pt]{article}

\input{preamble}

\usepackage[auth-sc,affil-sl]{authblk}

\newcommand{\coursewebpage}{\href{https://github.com/kapelner/QC_Math_390.4_Spring_2020}{course homepage}}



\title{MATH 390.4 (soon to be 342W) Spring 2020 \\ Course Syllabus}

\author[]{Adam Kapelner, Ph.D.}

\affil[]{Queens College, City University of New York}
\settimeformat{ampmtime}
\date{\small document last updated \today ~\currenttime }

\begin{document}
\maketitle

\begin{table}[htp]
\centering
\begin{tabular}{rl}
Instructor & Professor Adam Kapelner \\
%Office & 604 Kiely Hall \\
Contact & \url{kapelner@qc.cuny.edu} \\
Lecture Time / Loc & Tues and Thurs 11AM - 12:50PM / KY 283 \\
Required Lab Time / Loc & Friday 9-10:50AM KY 061 \\
My Office Hours / Loc & Tues and Thurs 12:50-1:30PM  / Kiely 283 \\
TA Office Hours / Loc & TBA  / Kiely 5th fl lounge\\
Course Homepage & \href{https://github.com/kapelner/QC_Math_390.4_Spring_2019}{https://github.com/kapelner/QC\_Math\_390.4\_Spring\_2020} \\
\end{tabular}
\end{table}

\section*{Course Overview}

MATH 342W. Data Science via Machine Learning and Statistical Modeling. 6 hr.; 4 cr. Prereq.: MATH 241 (intro to probablity and statistics), MATH 231 (intro to linear algebra), CSCI 111 (intro to programming) or equivalents. Recommended: ECON 382 (intro to economentrics) or equivalent, MATH 341 (Bayesian modeling), MATH 633 (statistical inference) or equivalent. Philosophy of modeling and learning using data. Prediction via the ordinary linear model including orthogonal projections, sum of squares identity, $R^2$ and RMSE. Polynomial and interaction regressions. Prediction with machine learning including neural nets (the perceptron), support vector machines and the tree methods CART, bagged trees and Random Forests. Probability estimation using logistic regression, asymmetric cost classifiers and the ROC / DET performance curves. Underfitting vs. overfitting and the bias-variance decomposition / tradeoff. Model validation including out of sample techniques such as cross validation and bootstrap validation. Correlation vs. causation, causal models, lurking variables and interpretations of linear model coefficients. Extrapolation. The \texttt{R} language will be taught formally from the ground and up (its use will be a substantial part of the homework) as well as visualization using the \texttt{ggplot} library and manipulation using the \texttt{dplyr} and \texttt{data.table} libraries. \pagebreak

You should be familiar with the following before entering the class:

\begin{itemize}
\itemsep -0.0em 
\item Basic Probability Theory: axioms, conditional probability, in/dependence
\item Modeling with discrete random variables: Bernoulli, Hypergeometric, Binomial, Poisson, Geometric, Negative Binomial, Uniform Discrete and others
\item Expectation and variance
\item Modeling with continuous random variables: Exponential, Uniform and Normal
\item Frequentist confidence intervals and hypothesis testing for one-sample proportions
\item Basic visualization of data: plots, histograms, bar charts
\item Linear algebra: Vectors, matrices, rank, transpose
\item Programming: basic data types, vectors, arrays, control flow (for, while, if, else), functions
\end{itemize}

\noindent We will review the above \textit{throughout the semester} when needed and we will do so rapidly. \\

\textbf{This is not your typical mathematics course.} This course will do lots of modeling of real-world situations using data via the \texttt{R} statistical language.

\section*{The 650.4 section}

You are the students taking this course as part of a masters degree in mathematics. Thus, there will be extra homework problems for you and you will be graded on a separate curve.

\section*{Course Materials}

We will be using many reference texts and three popular books which you will read portions from. However the main materials are the course notes. You should always supplement concepts from class by reading up on them online; \href{https://en.wikipedia.org}{wikipedia} I find the best for this. 

\paragraph{Theory Reference:} It is not necessary to have these two books, but it is recommended. The first is \qu{Learning from Data: A Short Course} by Abu-Mostafa, Magdon-Ismael and Lin which can be purchased used on \href{https://www.amazon.com/Learning-Data-Yaser-S-Abu-Mostafa/dp/1600490069}{Amazon}. We will also be using portions from \qu{Deep Learning} by Goodfellow, Bengio and Courville that can be purchased on \href{https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618}{Amazon} and read for free at \url{http://www.deeplearningbook.org/}.

\paragraph{Popular Books:} We will also be reading the non-fiction novel \qu{The Signal and the Noise} by Nate Silver which can also be purchased on \href{https://www.amazon.com/Signal-Noise-Many-Predictions-Fail-but/dp/0143125087}{Amazon}. This is \textit{required} --- you will have homework questions directly from this book. We will also be reading \qu{Preditive Analytics, Data Mining and Big Data} by Steven Finlay that can be purchased on \href{https://www.amazon.com/Predictive-Analytics-Data-Mining-Misconceptions/dp/1349478687}{Amazon} and it is also available online from the \href{https://link-springer-com.queens.ezproxy.cuny.edu/book/10.1057%2F9781137379283}{Queens College library system}. 

\paragraph{Computer Software:} You need your own personal computer, laptop preferred. We will be using \texttt{R} which is a free, open source statistical programming language and console available for all operating systems. Please download the latest version from: \url{http://cran.mirrors.hoobly.com/}. You will be expected to do programming. I recommend the IDE \texttt{RStudio} available for free at \url{https://www.rstudio.com/products/rstudio/download/}.

\paragraph{Source Control:} You will be expected to use \texttt{git} and have a \url{github.com} account with a repository named \texttt{QC\_MATH\_342}. You will use this repository to submit coding homework assignments (and theory assignments if you use \LaTeX).


\paragraph{Book on \texttt{R}:} We will be making use of \qu{R for Data Science} by Wickham and Grolemund which can be purchased on \href{https://www.amazon.com/R-Data-Science-Hadley-Wickham/dp/1491910399}{Amazon} or read online at \url{http://r4ds.had.co.nz/}.


\section*{Announcements}

Announcements will be made via email. I am known to send a couple emails per week on important issues. Thus, I will need the email address that you reliably check. The default is whatever is in CUNYfirst which many of you do not check. (See Homework \#0 for more information).

\section*{Classes}

There are 42 scheduled meetings. Of these, 25 will be lectures, 13 will be labs, 2 will be midterm exams which are in class and 2 will be review periods (the meeting before the exams). The exam schedule is given on page~\pageref{subsec:exam_schedule}.

I am \inred{canceling} Friday, April 17 (the Friday after the break which ends the day before) because many people will be away for the whole week returning the Monday after. We will do a makeup 2hr session at the end of the semester possibly during finals week to help with the projects. 

\subsection*{Lectures}

Lectures will be in KY258 and will besplit into two periods: theory and practice. The first is a standard chalkboard lectue where we learn concepts and the second will be using the computer/projector to see the concepts in action in the \texttt{R} language. I have a no computer / tablet / phone policy during the theory component of the lectures (only pen / pencil and paper) but you are highly recommended to have the laptop during the second part.

Below is a tentative schedule of the theory and practice topics covered by lecture number. 

\begin{enumerate}[(1)]
\item \textbf{Theory:} Review of syllabus, introducing science and modeling, definition of phenomena / the response $y$, reality vs. approximation, measurement vs. prediction and simulation, definition of learning from data, model validation, heuristics, ambiguous models, mathematical models, causal inputs, response spaces $\mathcal{Y}$, definition of features $\x$ and its feature space $\mathcal{X}$.

\textbf{Practice:} Short history of \texttt{R}, introduction to RStudio, arithmetic, assignment of variables, mathematical functions, logical operations, numeric / integer / boolean data types, vectors, sequences, subsetting, sorting, taxonomy of illegal values.

\item \textbf{Theory:} Feature spaces, binary, categorical and continuous data types, definition of metrics, ordinal codings, sample size $n$ vs. number of features $p$, error due to ignorance of information $\delta$, optimal response surface $f$, definition of training data $\mathbb{D}$, definition of candidate function set $\mathcal{H}$, definition of prediction function $g$, algorithms that produce prediction functions $\mathcal{A}$, definition of misspecification error, definition of optimal candidate function $h^*$, irreducible error $\mathcal{E}$, estimation error and residuals $e$.

\textbf{Practice:} Realizations of popular random variables, PDF / PMF / CDF / empirical CDF computations, quantile computations, factor-type variables, matrix data type and its critical functions, if / if-else / else / switch programmatic control, for / while / repeat loops, console printing, errors and warnings, try-catch control.

\item \textbf{Theory:} Visualization of training data $\mathbb{D}$, threshold models for classification, null model for classification $g_0$, concept of a parameter $\theta$ and parameter space $\Theta$, degrees of freedom, definition of objective function / error function, accuracy, sum of squared error (SSE), optimization within $\mathcal{A}$, linear threshold models, perception learning algorithm (PLA), introduction to neural networks.

\textbf{Practice:} Review of hashing and the list data type, the array data type for general tensors, naming for vectors / matrices / tensors, introduction to specifying functions, arguments, argument defaults, creating data matrices, tabling multiple features, the dataframe data type.

\item \textbf{Theory:} Review of lines in multiple dimensions with Hesse Normal Form, derivation of the support vector machine (SVM) using maximum margin objective, hinge error, SVM using the Vapnik objective, defintion of hyperparameters $\lambda$, $K$-nearest neighbors algorithm (KNN).

\textbf{Practice:} Installing and loading libraries from CRAN and githuv, public vs private functions and scoping, loading datasets from libraries / files / URLs, creating threshold models, writing the PLA, matrix operations: arithmetic / transpose / inverse / rank / trace, optimization algorithms e.g. Nelder-Mead, writing the KNN algorithm, using the SVM library and setting the hyperparameter.

\item \textbf{Theory:} Null model for regression $g_0$, linear models for continuous responses $\bbeta$, minimization of SSE for $p=1$ using basic calculus to arrive at the ordinary least squares (OLS) solution $\b$, review of covariance of two random variables $\sigma_{X,Y}$ and its estimate $s_{X,Y}$, review of correlation $\rho_{X,Y}$ and its estimate $r_{X,Y}$.

\textbf{Practice:} Computing sample covariance and correlation in the context of the OLS algorithm for $p=1$, computing OLS error metrics, the formula object, using the \texttt{lm} function, visualizing the OLS line atop a scatterplot, computing predictions in OLS, OLS in the boston housing data

\item \textbf{Theory:} Error metrics for regression: SSE, mean squared error (MSE) and root mean squared error (RMSE), approximate prediction confidence intervals using RMSE, sum of squares total (SST), concept of proportion of variance explained $R^2$, class of models with $R^2 < 0$, perfect fit models with $R^2 = 1$.

\textbf{Practice:} OLS on the Galton height data and the etymology of the word \qu{regression} in statistics, computing OLS in the case of categorical variables (ANOVA), dummifying variables, computing model matrices.

\item \textbf{Theory:} OLS estimates being the group averages with one binary feature, independence, dependence, association, correlation, OLS estimates with $p>1$, design matrix $X$, vector derivative properties: constant scalars, multiples, multiplication, quadratic forms, general OLS solution, review of matrix inverses, transposes, rank and symmetric matrices.

\textbf{Practice:} Visualizing $R^2$ for a model using error density estimation, computing general OLS estimates in multiple dimensions from scratch, making predictions using the \texttt{predict} interface for modeling.

\item \textbf{Theory:} OLS predictions as linear transformations, review of the linear algebra concepts of dimension, length, norm, subspace, linear in/dependence, column space, derivation of the orthogonal projection matrix for one dimension via law of cosines, outer products, idempotency.

\textbf{Practice:} Eigendecomposition, computing error metrics for general OLS, computing the null model.

\item \textbf{Theory:} Derivation of the orthogonal projection in multiple dimensions, equivalence of the OLS algorithm with orthongal projection, definition of the hat matrix $H$, review of the linear algebra concepts of eigenvectors and eigenvalues, computing the eigenvectors and eigenvectors of $H$.

\textbf{Practice:} Computing the hat matrix $H$ for the null model and in general, confirming its eigendecomposition and idempotency and rank, using $H$ to find the OLS predictions and residuals and verifying their orthogonality.

\item \textbf{Theory:} Verification of the symmetry and idempotency of $H$, proving that one multidimensional orthogonal projection is in general not the same as the sum of orthogonal projections in the component dimensions except if the component dimensions themselves are orthogonal, review of orthonormal matrices $Q$, proving the equivalence of $H = QQ^\top$, definition of $X = QR$ decomposition, the Gram-Schmidt algorithm, computing $R$, deriving the least squares estimate using $Q$ and $R$.

\textbf{Practice:} Computing $QR$ decomposition and confirming the orthonormality of $Q$, verifying the OLS predictions are the same with $Q$, writing the Gram-Schmidt algorithm, an overview of the piping / chaining concept in modern programming and in \texttt{R} with demos. 

\item \textbf{Theory:} Definition of sum of squares for the regression $SSR$, proving the sum of squares identity $SST = SSR + SSE$, showing that if $p$ increases by one dimension, then SSR is obligated to increase forcing $R^2$ higher, definition of overfitting in modeling, definition of chance capitalization, demonstrating that full overfitting in OLS leads to $H = I$, a superficial introduction to regularization (lasso regression and ridge regression).

\textbf{Practice:} Demo showing the iterative addition of a feature and $R^2$ monotonically increasing and RMSE monotonically decreasing (overfitting), demonstrating that random vectors are never truly orthogonal and thus their projections are non-zero, a lasso fit and a ridge fit of a dataset with $p \geq n$.

\item \textbf{Theory:} Definition of in-sample error metrics vs. out-of-sample (oos) error metrics, splitting $\mathbb{D}$ into $\mathbb{D}_{\text{train}}$ and $\mathbb{D}_{\text{test}}$ via split constant $K$, definition of \qu{honest} validation via oos error metrics, definition of the final model $g_{\text{final}}$ definition of underfitting, tracing the underfitting-overfitting complexity curve, definition of optimal-complexity models.

\textbf{Practice:} A more full demo of overfitting with visualizations, demo of consistency of OLS estimates, code to create train-test splits, demonstration that oos error is larger than in-sample error in the scenario of the model being overfit.

\item \textbf{Theory:} Definition of raw features versus derives features, increasing complexity in $\mathcal{H}$ using polynomial functions of raw features, interpretation of OLS coefficients $\b$, Weierstrauss Approximation Theorem, OLS with polynomial features, definition of the full rank Vandermonde matrix, definitions of interpolation vs. extrapolation.

\textbf{Practice:} Square, cube and higher order polynomial fitting both raw and orthogonal with visualization, overfitting with high-degree polynomials, demonstration of extrapolation in models of many different polynomial degrees, prediction with polynomial models, extrapolation in the Galton height data.

\item \textbf{Theory:} OLS using the log transformation on both features and response and interpretation of $\b$, derivation the log change is approximately percentage change, definition of first-order interactions in OLS, interpretations of coefficients in interaction models.

\textbf{Practice:} Log-linear model fitting and log-log linear model fitting, logging response to reduce the effect of influential observations, the grammar of graphics and the \texttt{ggplot} package to create histograms, scatterplots, box-whisker, violin plots, smoothing plots, overloading plots with many features, faceting, coloring, aesthetics and themes, using color illustrations and faceting to visualize potential first-order interactions in a linear model, fitting interaction models.

\item \textbf{Theory:} oos error metrics as estimates of model generalization error, sources of variance in these estimates, mitigation by adjusting $K$, further mitigation by using cross-validation (CV), $K$-fold CV and its aggregated error estimates, approximation confidence intervals for generalization error, discussion of reasonable values of $K$ in practice.

\textbf{Practice:} Simulating many different train-test splits to underscore that $K$ trades bias vs. variance in the generalization estimate, writing code for $K$-fold CV, using the package \texttt{mlr3} to automate $K$-fold CV.

\item \textbf{Theory:} Introduction of the fundamental problem of \qu{model selection} of candidates $g_1, \ldots, g_M$, model selection with honest validation via splitting $\mathbb{D}$ into $\mathbb{D}_{\text{train}}$, $\mathbb{D}_{\text{select}}$ and $\mathbb{D}_{\text{test}}$ via split constants $K_{\text{select}}$ and $K_{\text{test}}$, procedure to select best model among $M$ candidates and validation of the best model.

\textbf{Practice:} Writing code for the model selection procedure, review of basic \texttt{C++}, optimizing \texttt{R} code via the \texttt{Rcpp} package, benchmarking routines that require heavy looping between \texttt{Rcpp} and base \texttt{R}, benchmarking routines that require heavy recursion between \texttt{Rcpp} and base \texttt{R}.

\item \textbf{Theory:} Double-CV in the model selection procedure using inner folds and outer folds, discussion of reasonable values of $K_{\text{select}}$ and $K_{\text{test}}$ in practice, applying the model selection procedure to grid searching to locate the best value of hyperparameters $\lambda$ in algorithms that require $\lambda$, definition of stepwise modeling using the model selection procedure via the underfitting-overfitting complexity curve concept, stepwise OLS with a large basis of candidate terms. 

\textbf{Practice:} Using the package \texttt{mlr3} to automate the double-CV using inner and outer loops, using the package \texttt{mlr3} to automate the locating of optimal hyperparameters, demo of forward stepwise linear modeling and tracing the underfitting-overfitting complexity curve.

\item \textbf{Theory:} Definition of hyperrectangle basis for $\mathcal{X}$ and its OLS solution, unfeasibility of this algorithm in high $p$, introduction of the regression tree algorithm.

\textbf{Practice:} Binning model demo and visualization for varying bin sizes, introduction to data wrangling using the packages \texttt{dplyr} and \texttt{data.table}: filtering, sorting, grouping, summarizing, feature derivation, dataframe joining (left, right, inner, full, between / overlap), benchmarking the two libraries.

\item \textbf{Theory:} Full specification of regression tree algorithm: definition of a binary tree, definition of orthogonal-to-axes splits, nodes vs. leaves, left-right SSE weighting, leaf assignments, overfitting and tree-pruning.

\textbf{Practice:} Using the \texttt{YARF} package to produce regression trees, querying tree stats, visualizing trees and tree model predictions, tree differences by the pruning hyperparameter.

\item \textbf{Theory:} MSE of $g$ decomposition into bias and irreducible error for one $\mathbb{D}$, MSE of $g$ decomposition into bias, irreducible error and variance for multiple $\mathbb{D}$'s, MSE decomposition of $M$ different $g$'s averaged, strategies to eliminate bias and variance, non-parametric bootstrap sampling, Breiman's concept of bootstrap aggregation (bagging), correlation $\rho$ among the bootstrapped models, out-of-bag (oob) observations, validation in bagging via oob samples.

\textbf{Practice:} Visualizing $M$ bagged trees, demonstrating near zero bias, demonstrating variance reduction as $M \rightarrow \infty$, a comparison to OLS and high degree polynomial models, demonstration of generalization error improvement, demonstration of validation in bagging.

\item \textbf{Theory:} Demonstrating the bias for regression trees is near zero, reducing correlation among the bootstrapped models using feature sampling, introduction of random forests (RF) algorithm.

\textbf{Practice:} Demonstration that RF decreases $\rho$ and demonstration that it outperforms bagging in both regression and classification.

\item \textbf{Theory:} A basic discussion of \qu{causality} from a philosophical perspective, directed causal graphs, correlation vs. causation, incidental effects, lurking variables, spurious correlation, causation is defined by manipulation, a quick definition of randomized experimentation, real-world causal diagrams, wrong interpretations of $\b$ in OLS, the highly limited but true / complete paragraph-long interpretation of $\b$ in OLS, a discussion of how OLS regression accomplishes estimation of single features with other features \emph{ceteris paribus}.

\textbf{Practice:} Demos of whimsical spurious correlations, demonstration that spurious correlations are easy to find in simulation, a nice illustration of correlation without causation using an OLS model that reveals the true interpretation of $\b$.

\item \textbf{Theory:} Introduction of classification tree algorithm, the gini metric, leaf assignments, the two errors: false negatives and false positives, the 2$\times$2 confusion matrix and its metrics: precision, recall, accuracy, $F_1$ metric, false discovery rate, false omission rate.

\textbf{Practice:} Using the \texttt{YARF} package to produce classification trees, querying tree stats, visualizing trees and tree model predictions, tree differences by the pruning hyperparameter, measuring the two errors, computing confusion matrices and the other metrics.

\item \textbf{Theory:} Missing data mechanisms (MDMs): missing completely at random, missing at random, not missing at random with examples, strategies to handle missingness: listwise deletion, imputation, multiple imputation, miss forests algorithm and its convergence, the concept of \qu{retaining} missingness even after imputation, introduction of probability estimation using the independent bernoulli random variable model, optimal probability function, likelihood of $\mathbb{D}$, $\mathcal{H}$ for probability functions, generalized linear modeling, link functions: logistic / probit / complementary log-log, numerical approximations to the likelihood optimization.

\textbf{Practice:} An example of a dataset with missingness, assessing the different MDMs, listwise deletion, imputation and the \texttt{missForest} package for the recommended imputation, creating the missingness dummies as derives features in $X$.

\item \textbf{Theory:} Definition of logistic regression (LR), log-odds interpretation of LR $\b$, prediction in LR, error metrics for probability estimation: Brier and Log scoring rules, classification modeling from probability regression, optimal asymmetric cost modeling, response-operator curves (ROC), detection-error tradeoff curves (DET).

\textbf{Practice:} Fitting LR models using the \texttt{glm} package, predicting with LR models, validating creating asymmetric cost classifiers in LR models, constructing ROC and DET plots using LR models, locating optimal models with minimal cost. 
\end{enumerate}


\subsection*{Labs}

Labs  will be in KY061 (the computer lab) duiring the Friday morning meetings. Sometimes we will spend some of the two hours doing a practice lecture but the majority of this time will be your time. You will take turns \qu{driving} the coding in front of the class, working on exercises that you will finish for homework. Thus we will spend most a lot of time talking through problem solving skills in data science.


\subsection*{Lecture Upload}

As many previous students have noted, my handwritten notes are useful to me and not to many others. Thus, I will be rewarding students for taking notes, scanning them in and sending them to me. You will be rewarded in two ways: (1) if you do this for more than 10 lectures, you will be given the automatic 5 points (see grading policy on page \pageref{sec:grading}) for your classroom participation grade and (2) you have the option for me to say your name publicly on the \coursewebpage. Make sure you follow these instructions:

\begin{itemize}
\item You have \emph{one week only} from the time of the lecture to email me lecture notes.
\item There must be \emph{one} file and it must be in PDF format only.
\item The file must be $<$2MB. No exceptions.
\item Email it to me only at \texttt{kapelner@qc.cuny.edu}. I will not upload it otherwise.
\end{itemize}

If you send me your notes, you are (1) agreeing to the \href{https://github.com/kapelner/QC_Math_390.4_Spring_2020/blob/master/LICENSE}{MIT license} which means someone can freely copy your notes and even make money off of it (and not owe you a cent!) and (2) since github is mirrored, once your upload is on the web, it is there indeliby forever. \inred{If you are not comfortable with these two points, do not send me your notes.}

\section*{Homework}

Homework will be split into \textit{theory} and \textit{practice} (called \qu{labs}). This course will be the \qu{writing in the major course} next year. Thus, a portion of each theory and practice homework will involve writing \textit{English} and being graded on \textit{English}.

\subsection*{Theory Homework}

There will be 4-7 theory homework assignments. Homeworks will be assigned and placed on the \coursewebpage~ and will usually be due a week later in class. Homework will be \textbf{graded} out of 100 with extra credit getting scores possibly $> 100$. I will be doing the grading and will grade an \textit{arbitrary subset of the assignment} which is determined after the homework is handed in. Homework must be printed, neat and stapled (\textbf{it cannot be emailed to me}) but if you use \LaTeX~it can be pushed to your own repository (see \qu{Homework \LaTeX~bonus points} section below) and I will grade it digitally. Homework can be given to me in class or delivered under my office door (KY 604).

Graded homework will be returned in class. Regrades are handled during office hours or right after class is over. Scores for homeworks are finalized one week after the graded copies are handed back. Thereafter there will be no changes and no re-grading. Do not delay checking your graded homeworks. I am not perfect and I do make mistakes. It is your obligation to find these mistakes and report them.

You are encouraged to seek help from me if you have questions. After class and office hours are good times. \ingreen{You are highly recommended to work with each other and help each other.} \inred{You must, however, submit your own solutions, \textit{with your own write-up} and in \textit{your own words}. There can be no collaboration on the actual \textit{writing}. Failure to comply will result in severe penalties.} The university honor code is something I take seriously and I send people to the Dean every semester for violations.

\subsection*{Practice Homework (Labs)}

These will almost exclusively consist of short and medium coding exercises in \texttt{R}. Most of the assignment will be done for you and your peers during the Friday lab session.


\subsection*{Philosophy of Homework and Labs}


Homework is the \textit{most} important part of this course.\footnote{In one student's \href{http://www.ratemyprofessors.com/ShowRatings.jsp?tid=1951051}{observation}, I give a \qu{mind-blowing homework} every week.} Success in Statistics and Mathematics courses comes from experience in working with and thinking about the concepts. It's kind of like weightlifting; you have to lift weights to build muscles. My job as an instructor is to provide assistance through your \href{http://en.wikipedia.org/wiki/Zone_of_proximal_development}{zone of proximal development}. With me, you can grow more than you can alone. To this effect, homework problems are color coded \ingreen{green} for easy, \inyellow{yellow} for harder, \inred{red} for challenging and \inpurple{purple} for extra credit. You need to know how to do all the greens by yourself. If you've been to class and took notes, they are a joke. Yellows and reds: feel free to work with others. Only do extra credits if you have already finished the assignment. The \qu{[Optional]} problems are for extra practice --- highly recommended for exam study.

\subsection*{Time Spent on Homework }

This is a four credit course. Thus, the amount of work outside of the 4hr in-class time per week is 8-12 hours. I will aim for 10hr of homework per week on average. However, doing the homework well is your sole responsibility since I will make sure that by doing the homework you will study and understand the concepts in the lectures and you won't have all that much to do when the exams roll around.

\subsection*{Late Homework}

Late homework will be penalized 10 points per business day (Monday--Friday save holidays) for a maximum of five days. \textit{Do not ask for extensions}; just hand in the homework late. After five days, \textbf{you can hand it in whenever you want} until May 15 at noon. As far as I know, this is one of the most lenient and flexible homework policies in college. I realize things come up. Do not abuse this policy; you will fall far, far behind.

\subsection*{\LaTeX~Homework Bonus Points}

Part of good mathematics is its beautiful presentation. Thus, \ingreen{there will be a 1--7 point bonus} added to your theory homework grade  for typing up your homework using the \LaTeX ~typesetting system based on the elegance of your presentation. The bonus is arbitrarily determined by me.

I recommend using \href{http://overleaf.com}{overleaf} to write up your homeworks (make sure you upload both the hw\#.tex and the preamble.tex file). This has the advantage of (a) not having to install anything on your computer and thus not having to maintain your \LaTeX ~installation (b) allowing easy collaboration with others (c) alway having a backup of your work since it's always on the cloud. If you insist to have \LaTeX ~running on your computer, you can download it for Windows \href{http://www.miktex.org/download}{here} and for MAC \href{http://www.tug.org/mactex/}{here}. For editing and producing PDF's, I recommend \TeX works which can be downloaded \href{http://www.tug.org/texworks/#Getting_TeXworks}{here}. Please use the \LaTeX ~code provided on the \coursewebpage ~for each homework assignment. 

If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

Since this is extra credit, do not ask me for help in setting up your computer with \LaTeX~ in class or in office hours. Also, \textbf{never share your \LaTeX~code with other students} --- it is cheating and if you are found I will take it seriously.

\subsection*{Homework Extra Credit}

There will be many extra credit questions sprinkled throughout the homeworks. They will be worth a variable number of points arbitrarily assigned based on my perceived difficulty of the exercise. Homework scores in the 140's are not unheard of. They are a good boost to your grade; I once had a student go from a B to and A- based on these bonuses.


\subsection*{Homework \#0}

For your first homework, you must:

\begin{enumerate}[(1)]
\item email me at \href{kapelner@qc.cuny.edu}{kapelner@qc.cuny.edu} from the email address you wish to be contacted at for this course (most commonly this is a gmail address),
\item in the email, you must say \qu{My name is $<$Your Full Name as appears in the registrar$>$ and I have read and understand all the material in the course syllabus} and
\item provide a link to your public repository on github (this means you need to sign up for github first)
\end{enumerate}

\noindent \inred{This constitutes a contract --- you are agreeing to this syllabus.} \\

This assignment is due Friday, Jan 31, noon and will receive a grade of 0 or 100 with the usual 10 point penalty for lateness.

\section*{Writing Assignments}

There will be two writing assingments. (1) A \qu{philosophy of modeling} essay. Here you will coalesce the non-mathematical material that is crucial to this class. The purpose is to make you truly understand the modeling process and its limitations from start to finish. (2) A final project. Here you will use build a predictive model using a dataset. This is the capstone project for the entire data science and statistics major and it is where you will tie everything together.

This class will soon be the writing in the major course. Thus, writing is a major part of the curriculum herein.

\section*{Examinations}

Examinations are solely based on homeworks (which are rooted in the lectures)! If you can do all the green and yellow problems on the homeworks, the exams should not present any challenge. I will \textit{never} give you exam problems on concepts which you have not seen at home on one of the weekly homework assignments. 

Since the is the capstone course, there is no final exam, but a large final project. There will be two midterm exams and the schedule is below.

\subsection*{Exam and Major Assignment Schedule}\label{subsec:exam_schedule}

\begin{itemize}
\itemsep -0.0em 
\item Midterm examination I will be Thurs, March 12 in class with the first review session on the Tuesday prior
\item Midterm examination II will be Tues, May 12 in class with a review on the Friday prior
\item The philosophy of modeling paper is due Tues, Mar 24 at 11:59PM
\item The final project is due Sat, May 23 11:59PM by email
\end{itemize}

\subsection*{Exam Materials}

I allow you to bring any calculator you wish but it cannot be your phone. The only other items allowed are pencil and eraser. I do not recommend using pen but it is allowed.

I also allow \qu{cheat sheets} on examinations. For both midterms, you are allowed to bring \ingreen{two} 8.5'' $\times$ 11'' sheet of paper (front and back). \inred{Four sheets single sided are not allowed.} On this paper you can write anything you would like which you believe will help you on the exam. %For the final, you are allowed to bring three 8.5'' $\times$ 11'' sheet of paper (front and back). \inred{Six sheets single sided are not allowed.} I will be handing back the cheat sheets so you can reuse your midterm cheat sheets for the final if you wish. 

\inred{Food is not allowed during exams but beverages are allowed.}

%
%\subsection*{Cheating on Exams}
%
%If I catch you cheating, you can either take a zero on the exam, or you can roll the dice before the University Honor Council who may choose to suspend you.


\subsection*{Missing Exams}

There are no make-up exams. If you miss the exam, you get a zero. If you are sick, I need documentation of your visit to a hospital or doctor. Expect me to call the doctor or hospital to verify the legitimacy of your note. %If you need to leave the country for an emergency, I will expect proper documentation as well.

\subsection*{Special Services}

If you are a student who takes exams at the special services center, I need to see your blue slip one week before the exam to make proper arrangements with the center.

\section*{Class Participation (and attendance)}

I will be taking attendance selectively throughout the semester. Attendance counts towards the class participation portion of your grade in equal part with how often you ask and answer questions during the lecture.


\section*{Grading and Grading Policy}\label{sec:grading}

Your course grade will be calculated based on the percentages as follows: 

\begin{table}[h]
\centering
\begin{tabular}{l|l}
Theory Homework & 9\% \\
Labs & 13\% \\
Class participation & 5\% \\
Midterm Examination I & 18\%\\
Midterm Examination II & 18\%\\
Philosophy of Modeling Paper & 7\% \\
Final Project with Writeup & 30\%
\end{tabular}
\end{table}
\FloatBarrier

\noindent The second midterm is not cumulative. It only covers material \textit{after} midterm I.

%Each of the periods is assessed evenly. Thus, each period must count the same towards your grade. Since there is 75\% of the grade allotted to exams, there is 25\% allotted to each period. Thus, the final is upweighted towards the material covered in the third period. In summary, the final will have 5/35 points $\approx$ 14\% for the first period's material, 5/35 points $\approx$ 14\% for the second period's material and 25/35 points $\approx$ 71\% for the last period's material. A good strategy for the final is to just study the material after Midterm II and minimal studying for the previous material.

\subsection*{The Grade Distribution}

As this is a small and advanced class, the class curve will be quite generous. If you do your homework and demonstrate understanding on the exams, you should expect to be rewarded with an A or a B. $\leq$C's are for those who \qu{dropped out} somewhere mid-semester or who cannot demonstrate basic understanding.

\subsection*{Checking your grade and class standing}

You can always check your grades in real-time using the \href{http://gradesly.com}{grading site}. You will enter in your QC ID number (or email) and the password I will provide to you after HW \#0.

\section*{Auditing}

Auditors are welcome in both sections. They are encouraged to do all homework assignments. I will even grade them. Note that the university does not allow auditors to take examinations.


\end{document}
