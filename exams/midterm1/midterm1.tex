\documentclass[12pt]{article}

\include{preamble}

\title{Math 390.4 / 650.3 Spring 2020 \\ Midterm Examination One}
\author{Professor Adam Kapelner}

\date{Thursday, March 26, 2020}

\begin{document}
\maketitle

\noindent Full Name \line(1,0){410}

\thispagestyle{empty}

\section*{Code of Academic Integrity}

\footnotesize
Since the college is an academic community, its fundamental purpose is the pursuit of knowledge. Essential to the success of this educational mission is a commitment to the principles of academic integrity. Every member of the college community is responsible for upholding the highest standards of honesty at all times. Students, as members of the community, are also responsible for adhering to the principles and spirit of the following Code of Academic Integrity.

Activities that have the effect or intention of interfering with education, pursuit of knowledge, or fair evaluation of a student's performance are prohibited. Examples of such activities include but are not limited to the following definitions:

\paragraph{Cheating} Using or attempting to use unauthorized assistance, material, or study aids in examinations or other academic work or preventing, or attempting to prevent, another from using authorized assistance, material, or study aids. Example: using an unauthorized cheat sheet in a quiz or exam, altering a graded exam and resubmitting it for a better grade, etc.
\\

\noindent I acknowledge and agree to uphold this Code of Academic Integrity. \\

\begin{center}
\line(1,0){250} ~~~ \line(1,0){100}\\
~~~~~~~~~~~~~~~~~~~~~signature~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ date
\end{center}

\normalsize

\section*{Instructions}

This exam is 110 minutes (variable time per question) and closed-book. You are allowed \textbf{two} pages (front and back) of a \qu{cheat sheet} and scrap paper but no graphing calculator. Please read the questions carefully. No food is allowed, only drinks. %If the question reads \qu{compute,} this means the solution will be a number otherwise you can leave the answer in \textit{any} widely accepted mathematical notation which could be resolved to an exact or approximate number with the use of a computer. I advise you to skip problems marked \qu{[Extra Credit]} until you have finished the other questions on the exam, then loop back and plug in all the holes. I also advise you to use pencil. The exam is 100 points total plus extra credit. Partial credit will be granted for incomplete answers on most of the questions. \fbox{Box} in your final answers. Good luck!

\pagebreak

\problem [7min] This question is about modeling in general. 

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
%\item A model could be true.
\item A model could be false.
\item You can prove a model is true via simulation.
\item Only very accurate models can be used for prediction.
\item Mathematical models are never accurate enough to be useful.
\item Mathematical models built by learning from data require at least one feature. 
\item There can never be more features than observations when building a model by learning from data.
\item When building a model for a continuous response by learning from data, there must must be only continous features.
\item Values in a nominal feature can be coerced to numeric values.
\item Honest validation gives you an idea about how accurate your model is when using it for future prediction.
\item Validation can only be performed on mathematical models that were learned from data.
\end{enumerate}
\eenum\pagebreak


\problem [10min] This question is about creating a model learned from data and validating that model. Assume a dataset $\mathbb{D} := \angbraces{X, \y}$ where $X$ is an $n \times p$ matrix and $\y$ is an $n \times 1$ column vector. The dataset is split into a \text{train} and \text{test} set of $n_{\text{train}}$ observations and $n_{\text{test}}$ observations. Let $\mathbb{D}_{\text{train}} := \angbraces{X_{\text{train}}, \y_{\text{train}}}$ and $\mathbb{D}_{\text{test}} := \angbraces{X_{\text{test}}, \y_{\text{test}}}$ just like we did in class and lab by taking a random partition of the indices $1, 2, \ldots, n$. Assume $g = \mathcal{A}(\mathbb{D}_{\text{train}}, \mathcal{H})$ and $ g_{\text{final}} = \mathcal{A}(\mathbb{D}, \mathcal{H})$.

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
\item For all $\mathcal{A}$ we have studied, the model $g$ will be the same regardless of the partition of the indices that divides $\mathbb{D}$ into $\mathbb{D}_{\text{train}}$ and $\mathbb{D}_{\text{test}}$.
\item For all $\mathcal{A}$ we have studied, the model $g$ will be the same regardless of the order of the data in $\mathbb{D}_{\text{train}}$.
\item Honest validation provides an estimate to how $g$ will do in the future.
\item Honest validation provides an estimate to how $g_{\text{final}}$ will do in the future.
\item Assuming stationarity, comparing $g(X_{\text{train}})$ to $\y_{\text{train}}$ provides honest validation for the model $g$.
\item If stationarity cannot be assumed, comparing $g(X_{\text{train}})$ to $\y_{\text{train}}$ provides honest validation for the model $g$
\item Assuming stationarity, comparing $g(X_{\text{test}})$ to $\y_{\text{test}}$ provides honest validation for the model $g$.
\item If stationarity cannot be assumed, comparing $g(X_{\text{test}})$ to $\y_{\text{test}}$ provides honest validation for the model $g$.
\item If $\mathcal{Y} \subseteq \reals$, oos standard error of the residuals is given by the formula 

\beqn
\oneover{\sqrt{n_{\text{test}}}} \norm{\y_{\text{test}} - g(X_{\text{test}})}.
\eeqn
\item If $\mathcal{Y} = \braces{0, 1}$, then the oos misclassification rate is given by

\beqn
\oneover{{n_{\text{test}}}} \abss{\y_{\text{test}} - g(X_{\text{test}})}.
\eeqn
\end{enumerate}
\eenum\pagebreak

\problem [7min] A dataset of $n = 200$ and $p=1$ is collected. Here is a plot of the raw $\mathbb{D}$:

\beqn
\centering\includegraphics[width=6in]{parabola.pdf}
\eeqn

\noindent Let $X$ be the random variable (r.v.) that realized $x$ and let $Y$ be the r.v. that realized $y$.

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
\item $X$ and $Y$ are likely independent.
\item $X$ and $Y$ are likely dependent.
\item $X$ and $Y$ are likely associated.
\item $X$ and $Y$ are likely not associated.
\item $X$ and $Y$ likely have covariance zero.
\item $X$ and $Y$ likely have covariance nonzero.
\item $X$ and $Y$ likely have correlation zero.
\item $X$ and $Y$ likely have correlation nonzero.
\item If all values of $x > 0$ were dropped from $\mathbb{D}$, then it would appear that $X$ and $Y$ likely have covariance positive.
\item If all values of $x > 0$ were dropped from $\mathbb{D}$, then it would appear that $X$ and $Y$ likely have covariance negative.
\end{enumerate}
\eenum\pagebreak

\problem [13min] The raw $\mathbb{D}$ with $n = 27$ is plotted below where $x_1$ is on the horizontal axis and $x_2$ is on the vertical axis. The binary response $y$ measures patient outcome and is depicted by different shapes (see the illustration's legend).

\beqn
\centering\includegraphics[width=5in]{nonlinsep.pdf}
\eeqn

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
\item $x_1$ is most likely an ordinal variable.
\item $x_1$ is most likely a nominal variable.
\item If $\mathcal{A} =$ OLS, then it would not be able to return a $g$.
\item If $\mathcal{A} =$ perceptron learning algorithm without a limit on iterations, then it would not be able to return a $g$.
\item If $\mathcal{A} =$ SVM with the Vapnik function, then you would need to specify the value of $\lambda$ to be able to return a $g$.
\item If $\mathcal{A} =$ SVM with the Vapnik function, then $g$ would have zero average hinge error.
\item If $\mathcal{A} =$ SVM with the Vapnik function and $\lambda = 0$, then $g$ would divide $\mathbb{D}$ only by using $x_1$.
\item Regardless of the $\mathcal{A}$ used, $R^2$ would be a preferred metric to assess model accuracy.
\item It is possible to design an $\mathcal{A}$ that could return a model $g$ that gives a perfect fit.
\item It $\mathcal{A} =$ KNN where $K = n$, then $g(\x)$ will be the same for all observations $\x \in \mathbb{D}$.
\end{enumerate}
\eenum\pagebreak

\problem [15min] A raw $\mathbb{D}$ is plotted below:

\beqn
\centering\includegraphics[width=5in]{curvyline.pdf}
\eeqn

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
\item $\mathcal{X}$ is likely $\bracks{-2,2}$.
\item $\mathcal{Y}$ is likely $\subseteq \reals$.
\item If $\mathcal{H} = \braces{a~:~a \in \reals}$ then a rational $\mathcal{A}$ would return $g$ where $a \approx 0$.
\item If $\mathcal{H} = \braces{a + b \indic{x > 0}\,:\,a,b \in \reals}$ then it is likely $f \in \mathcal{H}$.
\item If $\mathcal{H} = \braces{a + b \indic{x > 0}\,:\,a,b \in \reals}$ then a rational $\mathcal{A}$ would return $g$ where $b > a$.
\item If $\mathcal{H} = \braces{a + b \indic{x > 0}\,:\,a,b \in \reals}$ and $\mathcal{A} =$ OLS, then it would be impossible to compute $R^2$ for $g$.
\item If $\mathcal{H} =$ the space of all continuous functions, then $t \in \mathcal{H}$.
\item If $\mathcal{H} = \braces{a + b x\,:\,a,b \in \reals}$ then a rational $\mathcal{A}$ would return $g$ where $a < 0$.
\item With $\mathcal{H}$ specified optimally, the error due to estimation will likely be low.
\item With $\mathcal{H}$ specified optimally, the error due to misspecification will likely be low.
\end{enumerate}
\eenum\pagebreak



\problem [17min] Let $\X = \bracks{\onevec_n~|~\x_1~|~\ldots~|~\x_p} \in \reals^{n \times (p +1)}$ and $\rank{\X} = p + 1$ and $\y \in \reals^n$. Assume that the vectors $\x_1, \ldots, \x_p$ are mean-centered (i.e. the sample average of all their entries is zero) and that $\y$ is also mean-centered. Your modeling task is to model the response using the $n$ observations. Your $\mathcal{A} =$ OLS. Let $\b$ be the vector of OLS estimates for the $p+1$ features, let $\bbeta$ be the slope coefficients in the optimal linear model, $\H$ be the orthogonal projection matrix onto the $\colsp{\X}$, $\yhat$ is the vector of predictions for the $n$ observations and $\e$ are the residuals.

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
\item $\b = \displaystyle\argmin_{\w \in \reals^{p + 1}}\braces{(\y - \X\w)^\top (\y - \X\w)}$.
\item $\XtX$ is a full rank $n \times n$ matrix.
\item $\H$ is a full rank $n \times n$ matrix.
\item $\H\e = \zerovec_n$.
\item $\H\X\b - \yhat = \zerovec_n$.
\item $\H\X\bbeta - \yhat = \zerovec_n$.
\item $\onevec_n^\top\H\onevec_n = p + 1$.
\item $\x_2^\top\H\x_2 = (n - 1)s^2_{x_2}$.
\item $\rank{\I - \H} = n - p - 1$.
\item If $p = 0$ then $\yhat = \zerovec_n$.
\end{enumerate}
\eenum\pagebreak



\problem [13min] We use the same setup as in Problem 6. \ingray{Let $\X = \bracks{\onevec_n~|~\x_1~|~\ldots~|~\x_p} \in \reals^{n \times (p +1)}$ and $\rank{\X} = p + 1$ and $\y \in \reals^n$. Assume that the vectors $\x_1, \ldots, \x_p$ are mean-centered (i.e. the sample average of all their entries is zero) and that $\y$ is also mean-centered. Your modeling task is to model the response using the $n$ observations. Your $\mathcal{A} =$ OLS. Let $\b$ be the vector of OLS estimates for the $p+1$ features, let $\bbeta$ be the slope coefficients in the optimal linear model, $\H$ be the orthogonal projection matrix onto the $\colsp{\X}$, $\yhat$ is the vector of predictions for the $n$ observations and $\e$ are the residuals.} But now we progressively add columns consisting of entries of random noise to the matrix $\X$ which remains full rank each time a column is appended.

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter. \\
~\\
With each additional column appended, ...

\begin{enumerate}[(a)]
\item SST decreases.
\item $\normsq{\yhat}$ increases.
\item $\dim\bracks{\H}$ remains the same.
\item $\rank{\H}$ remains the same.
\item the RMSE increases.
\item the generalization error of the model increases.
\item each residual's absolute value will decrease.
\item each slope coefficient (the entries in $\b$) will decrease.
\item the angle between $\yhat$ and $\y$ gets closer to zero.
\item with $p > n$, $\mathcal{A}$ fails to run.
\end{enumerate}
\eenum\pagebreak

\problem [12min] We use the same setup as in Problem 6. \ingray{Let $\X = \bracks{\onevec_n~|~\x_1~|~\ldots~|~\x_p} \in \reals^{n \times (p +1)}$ and $\rank{\X} = p + 1$ and $\y \in \reals^n$. Assume that the vectors $\x_1, \ldots, \x_p$ are mean-centered (i.e. the sample average of all their entries is zero) and that $\y$ is also mean-centered. Your modeling task is to model the response using the $n$ observations. Your $\mathcal{A} =$ OLS. Let $\b$ be the vector of OLS estimates for the $p+1$ features, let $\bbeta$ be the slope coefficients in the optimal linear model, $\H$ be the orthogonal projection matrix onto the $\colsp{\X}$, $\yhat$ is the vector of predictions for the $n$ observations and $\e$ are the residuals.} But now we use Q-R decomposition to find matrices such that $\X = \Q\R$ where $\q_j$ denotes the $j$th column of $\Q$.

\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter. 

\begin{enumerate}[(a)]
\item The dimension of $\Q$ is the same as $\X$.
\item $\colsp{\X} = \colsp{\Q}$.
\item $\R$ is a full rank $n \times n$ matrix.
\item $\R$ cannot be inverted.
\item $\y \in \colsp{\Q}$.
\item $\yhat \in \colsp{\Q}$.
\item The first column of $\Q$ is a scalar multiple of $\onevec_n$.
\item The last column of $\Q$ is a scalar multiple of $\x_p$.
\item $\H = \Q\Q^\top$.
\item $\yhat = \proj{\q_1}{\y} + \proj{\q_2}{\y} + \ldots + \proj{\q_{p+1}}{\y}$
\end{enumerate}
\eenum\pagebreak


\problem [14min] A dataset of $n = 40$ and $p=1$ is collected. Here is a plot of the raw $\mathbb{D}$:

\beqn
\centering\includegraphics[width=6in]{binary.pdf}
\eeqn

And here is some \texttt{R} code that uses the raw $\mathbb{D} = \angbraces{\x,\y}$ with its output:

\begin{lstlisting}
> round(summary(y[x == 0]), 2)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  -0.54    1.47    1.91    1.86    2.72    3.63 
> round(summary(y[x == 1]), 2)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   2.46    4.47    4.91    4.86    5.72    6.63 
> mod = lm(y ~ x)
\end{lstlisting}
\vspace{-1cm}
\benum

\subquestionwithpoints{10} Record the letters of all the following that are \textbf{true}. Your answer will consist of a string (e.g. \texttt{aebgd}) where the order of the letters does not matter.

\begin{enumerate}[(a)]
\item $x$ is termed a ``dummy variable''.
\item Modeling the response $y$ here is termed a \qu{classification problem}.
\item The model $g$ in the object \texttt{mod} is a linear model.
\item The $R^2$ is likely to be low for the model $g$ in the object $\texttt{mod}$.
\item The $R^2$ is likely to be high for the model $g$ in the object $\texttt{mod}$.
\item When \texttt{coef(mod)} is called, it returns $b_1 = 4.86$.
\item When \texttt{coef(mod)} is called, it returns $b_1 = 3.00$.
\item If \texttt{mod = lm(y $\sim$ 0 + x)} and \texttt{coef(mod)} is called, it returns $b_1 = 4.86$.
\item If \texttt{mod = lm(y $\sim$ 0 + x)} and \texttt{coef(mod)} is called, it returns $b_1 = 3.00$.
\item The model $g$ in the object $\texttt{mod}$ can only predict for $x \in \braces{0,1}$.
\end{enumerate}
\eenum\pagebreak


\end{document}

\problem We are trying to predict heart attacks based on two risk factors: blood pressure and cholesterol. 

Blood pressure levels are based on the classification recommended by the Seventh Report of the Joint National Committee on Prevention, Detection, Evaluation and Treatment of High Blood Pressure and are defined as follows: normal (systolic blood pressure $<$120 mm Hg and a diastolic blood pressure $<$80 mm Hg); pre-hypertension (systolic blood pressure 120--139 mm Hg or diastolic blood pressure 80--89 mm Hg); hypertension stage 1 (systolic blood pressure 140--159 mm Hg or diastolic blood pressure 90--99 mm Hg); and hypertension stage 2 (systolic blood pressure $>$160 mm Hg or diastolic blood pressure $>$100 mm Hg). Persons are classified into the higher blood pressure group if the systolic and diastolic values fall within more than one category. Cholesterol is measured as the LDL cholesterol. 

Below is a plot of some historical pilot data.

\begin{figure}[htp]
\centering
\includegraphics[width=6in]{nonlinsep}
\end{figure}

\benum
\subquestionwithpoints{2} Based on the description in the problem header label the axes  above. \spc{-0.5}


\subquestionwithpoints{2} If we are going to build a model from this data, what type of statistical learning are we doing? Circle all that apply.

\begin{enumerate}[i)]
\item regression to predict $y$
\item binary classification to predict $y$
\item multiclass (i.e. specifically non-binary) classification to predict $y$
\item finding $t$ directly
\item finding optimal $n$ and $p$ for $\mathbb{D}$
%\item building $\mathcal{A}$ to find $f$ directly
%\item estimating $\mathcal{X}$ and $\mathcal{Y}$ using $\mathcal{H}$
\item supervised learning
\item unsupervised learning
%\item OLS
\end{enumerate}

\subquestionwithpoints{2} Assume we are doing supervised learning. Relabel the axes and legend above using parentheses around $x_j$ and $y$ where $j$ is the index on the variable number which you need to determine. For example, an axis may be labeled \qu{\# of phone calls ($x_{17}$)}.

\subquestionwithpoints{2} Is this data linearly separable? Yes / no and explain your answer.\spc{2}

%\subquestionwithpoints{2} For the $x_j$'s and $y$ you named in the previous question, what are their types?\spc{2}

\subquestionwithpoints{2} Denote $\mathbb{D} := <X,\y>$. What is $\dim\bracks{X}$?

\subquestionwithpoints{2} Given $\mathbb{D}$, find $g_0$, the null model.\spc{-0.5}




\subquestionwithpoints{2} Let $\mathcal{A} = $ perceptron learning algorithm. What will its output be? Circle all that apply.

\begin{enumerate}[i)]
\item $\hat{y}$
\item $\mathcal{A}$
\item $g$
\item $h^*$
\item $x_{\cdot 1}, \ldots, x_{\cdot p}$
\item $f$
\item $z_1, \ldots, z_t$
\end{enumerate}

\subquestionwithpoints{4} Let $\mathcal{A} = $ perceptron learning algorithm. Starting the algorithm at the zero vector of the appropriate dimension, draw the algorithm's output line on the plot as a dotted line or explain below why you are unable to do so.\spc{3}


\subquestionwithpoints{2} Let $\mathcal{A} = $ an algorithm that minimizes average hinge loss. Write the objective function of the algorithm below.\spc{3}

\subquestionwithpoints{3} Let $\mathcal{A} = $  SVM with the Vapnik objective function with a $\lambda$ specified to be small but non-zero. Draw the algorithm's output line on the plot below or explain below why you are unable to do so.


\begin{figure}[htp]
\centering
\includegraphics[width=6in]{nonlinsep_trace}
\end{figure}

%\subquestionwithpoints{2} Is this $\mathcal{A}$ the same as using the SVM with the Vapnik objective function while setting $\lambda$ to be trivially small or zero? Yes / no.\spc{-0.5}

\subquestionwithpoints{2} What is the average classification error of the model in the previous question?\spc{1}

\subquestionwithpoints{2}  What is the approximate $R^2$ of this fit?

\begin{enumerate}[i)]
\item $<$0\%
\item 0\%
\item 2.5\%
\item 25\%
\item 95\%
\item 100\%
\item $\mathcal{A}$ cannot produce an output thus $R^2$ cannot be estimated.
\end{enumerate}

\subquestionwithpoints{2}  Why is $R^2$ an inappropriate metric to be using here to measure model performance?\spc{4}

%\subquestionwithpoints{2} Draw a $p=1$ threshold model below that has minimum average misclassification error.
%
%
%\begin{figure}[htp]
%\centering
%\includegraphics[width=6in]{nonlinsep_trace}
%\end{figure}



\subquestionwithpoints{2} Draw a model below that has zero average hinge error.


\begin{figure}[htp]
\centering
\includegraphics[width=6in]{nonlinsep_trace}
\end{figure}


\subquestionwithpoints{4} Let $\mathcal{A} = $ KNN with $d = $ Euclidean distance. Evaluate the following:

\begin{enumerate}[i)]
\item $K$ = 1, $g(4,145) = $
\item $K$ = 1, $g(3,125) = $ 
\item $K$ = 4, $g(3,125) = $ 
\end{enumerate}

\subquestionwithpoints{5} Is there a problem with using  $\mathcal{A} = $ perceptron, $\mathcal{A} = $ SVM and $\mathcal{A} = $ KNN with $\mathbb{D}$? Yes / no. Discuss.\spc{10}
\eenum

\problem The following dataset is a mock view of a financial asset. The $x$ axis represents time and the $y$ axis represents value. There are $n = 300$ data points and $s^2_x = 3.322$ and $s^2_y = 3.196$.

\begin{figure}[htp]
\centering
\includegraphics[width=5in]{curvyline}
\end{figure}


\benum

\subquestionwithpoints{2} Estimate $\mathcal{X}$ and $\mathcal{Y}$.\spc{0.5}
\subquestionwithpoints{2} Estimate the equation for $g_0$.\spc{0}

\subquestionwithpoints{2} Estimate the $R^2$ for $g_0$.\spc{0}

%\subquestionwithpoints{2} If $\mathcal{A} = $ OLS, estimate the equation for $g$.\spc{1}

\subquestionwithpoints{5} If $\mathcal{A} = $ OLS, estimate the percentage of the RMSE of $g$ belonging to each of the three errors. 

\begin{enumerate}[i)]
\item Name of error:\\
~\\
Percentage of RMSE:\\
\item Name of error:\\
~\\
Percentage of RMSE:\\
\item Name of error:\\
~\\
Percentage of RMSE:\\
\end{enumerate}


%\subquestionwithpoints{2} If $\mathcal{A} = $ OLS, and $\x$ (the vector of all $x_i$'s) were drawn from the r.v. model $X$ and $\e$ (the vector of all $e_i$'s) were drawn from the r.v. model $E$, do you think $X$ and $E$ would be independent? Explain why or why not. \spc{2}

\subquestionwithpoints{4} Let $\mathcal{A} = $ minimize the least squares error but provide a better $\mathcal{H}$ than the set $\braces{w_0 + w_1 x~:~w_0, w_1 \in \reals}$ where \qu{better} means that the elements $h$ can much better approximate $f$. \spc{1}


\subquestionwithpoints{2} Draw $h^*$, an element of your set in (d) on the plot below:

\begin{figure}[htp]
\centering
\includegraphics[width=5in]{curvyline}
\end{figure}


\subquestionwithpoints{5} Compute the $R^2$ of the model depicted as the solid line below \emph{as best as you can}.

\begin{figure}[htp]
\includegraphics[width=5in]{curvyline_with_f}
\end{figure}~\spc{4}


\eenum

\problem We will now be looking at the \texttt{diamonds} dataset. Below is some \texttt{R} code that gives background on this data frame which will be referenced throughout this problem. The response variable that is usually modeled is \texttt{price}. This problem contains some coding exercises. 

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
> diam = ggplot2::diamonds
> dim(diamonds)
[1] 53940    10
> summary(diamonds)
     carat               cut        color        clarity          depth      
 Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  
 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  
 Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  
 Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  
 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  
 Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  
                                    J: 2808   (Other): 2531                  
     table           price             x                y                z         
 Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720   1st Qu.: 2.910  
 Median :57.00   Median : 2401   Median : 5.700   Median : 5.710   Median : 3.530  
 Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735   Mean   : 3.539  
 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540   3rd Qu.: 4.040  
 Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900   Max.   :31.800
\end{lstlisting}

\lstset{basicstyle=\footnotesize}

\benum

\subquestionwithpoints{2} In this $\mathbb{D}$, what is $n$ and $p$? \spc{0}

\subquestionwithpoints{2} What is the type of the variable \texttt{carat}? \spc{0}

\subquestionwithpoints{2} What is the type of the variable \texttt{cut}? \spc{0}

\subquestionwithpoints{2} If you were fitting an OLS model of \texttt{price} using \texttt{color}, what would $p$ be in that model? \spc{0}

\subquestionwithpoints{2} Write code that extracts every 50th diamond observation. \spc{1}

\subquestionwithpoints{3} Write code that adds a new variable to the data frame named \texttt{customer\_favorite} that is 1 if the \texttt{cut} is \texttt{ideal} and the \texttt{color} is either \texttt{G} or \texttt{J} and the \texttt{depth} is 90\%ile or above. \spc{3}

\subquestionwithpoints{3} Assume that \texttt{x}, \texttt{y} and \texttt{z} are the spatial dimensions of the stone. Write code below that creates a data frame called \texttt{tinies} that contains diamonds that have volume less than 50. \spc{0.5}

\subquestionwithpoints{3} Describe the output of this script as completely as possible.

\begin{lstlisting}
> dict = list()
> for (color in unique(diam$color)){
  dict[[color]] = diam[diam$color == color, "price"]
}
> dict[["D"]]
\end{lstlisting}~\spc{1.5}

\eenum


\problem This last problem contains a pure coding exercise. 

\benum

\subquestionwithpoints{5} Complete the function below to spec. You don't have to use all the free lines given (in fact, it can be done in one line). You are free to use the \texttt{mean}, \texttt{sd}, \texttt{cov}, \texttt{cor} and other base \texttt{R} functions (but you cannot use \texttt{lm}). \spc{-0.5}

\begin{lstlisting}
#' This function implements the linear least squares regression algorithm
#' for one covariate popularized by Sir Francis Galton in 1886.
#'
#' @param x		the continuous predictor
#' @param y		the continuous response 	
#' @return 		a list containing a key ``b_0'' whose value is the inter-
#'						cept, a key ``b_1'' whose value is the slope, a key ``Rsq''
#'						that is the R-squared of the fit.
linear_least_squares_algorithm = function(x, y){
  b_1 = cor(x, y) * sd(y) / sd(x)
  b_0 = mean(y) - b_1 * mean(x)











}
\end{lstlisting}

\eenum
\end{document}

