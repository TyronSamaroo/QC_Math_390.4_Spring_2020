---
title: "Lab 10"
author: "Your Name Here"
output: pdf_document
date: "11:59PM May 9, 2020"
---

Before you do this lab, ensure you have the Java JDK installed. The JDK is NOT the JRE. The former allows you to compile Java programs and the latter allows you only to run Java programs. Then insure that `rJava` is installed and working. In other words, the following should work and give the same output from practice lecture 12. If it doesn't, try the code that is commented out to reinstall. Google errors. Frustration in libraries and platforms not working on your computer is unfortunately part of computer science and thus part of data science.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(rJava)
#if that doesn't work, use:
# install.packages("rJava", type = "source")
# library(rJava)
.jinit() #this initializes the JVM in the background and if this runs with no issues nor output, you probably have rJava installed and connected to the JDK properly.
java_double = .jnew("java/lang/Double", 3.1415)
java_double
class(java_double)
.jclass(java_double)
#call an instance method 
.jcall(java_double, "I", "intValue") #java_double.intValue();
#call a static method
J("java/lang/String", "valueOf", java_double) #String.valueOf(java_double);
J("java/lang/String", "valueOf", x) #some sort of alphanumeric code for the pointer address
```

It is important to have rJava working on your computer as a fair number of R packages really do make use of it. It's a good thing to have in your toolbox in general.

Now ensure that YARF is installed properly:


```{r}
# pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
# pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev")
pacman::p_load(YARF)
```

If that printed out "YARF can now make use of [n] cores", you are in business.

Let's take a look at the simulated sine curve data from practice lecture 12:

```{r}
rm(list = ls())
pacman::p_load(tidyverse, magrittr)
n = 500
x_max = 10
x = runif(n, 0, x_max)
y = sin(x) + rnorm(n, 0, 0.3)
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd = 0.6) 
```

Use `mlr` and `YARF` to locate the optimal node size hyperparameter for the regression tree model.

```{r}
#TO-DO
```

Plot the regression tree model with the optimal node size.

```{r}
#TO-DO
```

First load the tree-building package:

```{r}
options(java.parameters = "-Xmx4000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
pacman::p_load(YARF)
```

Let's take a look at the simulated sine curve data (i.e. the illustration I drew on the board last class)

```{r}
pacman::p_load(tidyverse)
n_train = 500
x_max = 10
x_train = runif(n_train, 0, x_max)
y_train = sin(x_train) + rnorm(n_train, 0, 0.3)
ggplot(data.frame(x = x_train, y = y_train), aes(x, y)) + geom_point(lwd=0.6) 
```

create a test set from the this data generating process with size 1000.

```{r}
n_test = 1000
x_test = runif(n_test, 0, x_max)
y_test = sin(x_test) + rnorm(n_test, 0, 0.3)
ggplot(data.frame(x = x_test, y = y_test), aes(x, y)) + geom_point(lwd=0.6) 
```


Fit a linear model to this dataset and test out of sample to get an idea of the generalization error.

```{r}
linear_mod = lm(y_train ~ x_train)
se_oos = sd(y_test - predict(linear_mod, data.frame(x_train = x_test)))
se_oos
```

Fit a tree to this dataset where nodesize is 25.

```{r}
tree_mod = YARFCART(data.frame(x = x_train), y_train, nodesize = 25)
```

How many nodes and how deep is this tree?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Create an image of this tree's nodes and split rules.

```{r}
illustrate_trees(tree_mod, max_depth = 4)
```

Test this tree model's performance out of sample to get an idea of the generalization error.

```{r}
se_oos = sd(y_test - predict(tree_mod, data.frame(x = x_test)))
se_oos
```

Fit a tree to this dataset where nodesize is 1 and test out of sample.

```{r}
tree_mod = YARFCART(data.frame(x = x_train), y_train, nodesize = 1)
get_tree_num_nodes_leaves_max_depths(tree_mod)
se_oos = sd(y_test - predict(tree_mod, data.frame(x = x_test)))
se_oos
```

Create M = 200 bootstrap samples of the data and save in a list.

```{r}
M = 200
bootstrap_x_train = list()
bootstrap_y_train = list()

for(i in 1:M){
  bootstrap_indices = sample(1 : n_train, replace = TRUE, size = n_train)
  bootstrap_x_train[[i]] = x_train[bootstrap_indices]
  bootstrap_y_train[[i]] = y_train[bootstrap_indices]
}
```

Create a bag of M trees model where nodesize = 5 (the regression default). Use the call of `YARFCART`.

```{r}
tree_mods = list()
for (k in 1 : M){
  tree_mods[[k]] = YARFCART(data.frame(x = bootstrap_x_train[[k]]), bootstrap_y_train[[k]], nodesize = 5, calculate_oob_error = FALSE)
}
```

Test this bagged model out of sample.

```{r}
y_test_hats = matrix(NA, nrow = n_test, ncol = M)

for(k in 1 : M){
  y_test_hats[, k] = predict(tree_mods[[k]], data.frame(x = x_test))
  
}
y_test_hats = rowMeans(y_test_hats)

se_oos = sd(y_test - y_test_hats)
se_oos
```

Using the bootstrapped samples, find the oob error. This is hard!

```{r}
#TO-DO
```

Fit a random forest model (RF) to the data. Report oob error.

```{r}
#TO-DO
```

Test the RF model out of sample. Is this error lower than the bagged model? Is the error similar to its oob error?

```{r}
#TO-DO
```

Load the `diamonds' dataset. Sample 1,000 rows for training and 1,000 rows for testing.

```{r}
#TO-DO
```

Build a linear model and test.

```{r}
#TO-DO
```

Build a bagged model and test. You can use `YARFBAG`.

```{r}
#TO-DO
```

Build a RF model and test. You can use `YARF`.


```{r}
#TO-DO
```

Explain why the gains are small from linear regression -> bagged trees -> random Forests

#TO-DO

Use `mlr` to build a RF model that is optimally tuned for the hyperparameter `mtry` and test out of sample.

```{r}
#TO-DO
```

Load the `nycflights13` data and join the weather table to airport in the four ways we learned about. 


```{r}
#TO-DO
```

Explain in English what each of these joins is doing.

#TO-DO
