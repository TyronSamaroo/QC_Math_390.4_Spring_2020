---
title: "Lab 9"
author: "Your Name Here"
output: pdf_document
date: "11:59PM May 2, 2020"
---

Load the `adult` dataset and remove missingness.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult)
```

We had problems with the features "occupation" and "native_country". Go through these two features and and identify levels with too few examples and wrap them into a level called "other". This is standard practice.

```{r}
sort(table(adult$occupation))
adult$occupation = as.character(adult$occupation)
adult$other = adult$occupation %in% c("Armed-Forces", "Priv-house-serv", "Protective-serv")
table(adult$other)
adult$occupation[adult$other] = "other"
adult$occupation = as.factor(adult$occupation)
sort(table(adult$occupation))

sort(table(adult$native_country))
adult$domestic = ifelse(adult$native_country == "United-States", 1, 0)
table(adult$domestic)
adult$native_country = NULL
```

We will be doing model selection. Split the dataset into 3 distinct subsets of 5000 observations. You will need the following variables: `Xtrain`, `ytrain`, `Xselect`, `yselect`, `Xtest`, `ytest`.

```{r}
set.seed(1)
adult = adult[sample(1 : nrow(adult)), ]
Xtrain = adult[1 : 5000, ]
Xtrain$income = NULL
ytrain = ifelse(adult[1 : 5000, "income"] == ">50K", 1,0)
Xselect = adult[5001 : 10000, ]
Xselect$income = NULL
yselect = ifelse(adult[5001 : 10000, "income"] ==">50K", 1,0)
Xtest = adult[10001 : 15000, ]
Xtest$income = NULL
ytest = ifelse(adult[10001 : 15000, "income"] == ">50K",1,0)
 
```

Fit a vanilla logistic regression on the training set.

```{r}
logistic_mod = glm(ytrain ~ ., Xtrain, family = "binomial")
```

and report the log scoring rule, the Brier scoring rule.

```{r}
phatTrain = predict(logistic_mod, Xtrain, type = 'response')
logScoringRule = ytrain * log(phatTrain)+(1-ytrain)*log(1-phatTrain)
head(logScoringRule)
mean(logScoringRule)
mean(-(ytrain - phatTrain)^2)
```

Then use this probability estimation model to do classification by thresholding at 0.5. Tabulate a confusion matrix and compute the misclassification error.

```{r}
y_hat_train = ifelse(phatTrain >= 0.5, 1, 0)
table(ytrain, y_hat_train)

mean(ytrain!=y_hat_train)

```

We will be doing model selection using a basis of linear features consisting of all interactions of the 14 raw features. Create a model matrix from the training data containing all these features. Make sure it has an intercept column too (the one vector is usually an important feature).

```{r}
Xmm_train = model.matrix(~.*. , Xtrain)
Xmm_select = model.matrix(~.*. , Xselect)
dim(Xmm_train)
dim(Xmm_select)
```

Write code that will fit a model stepwise. You can refer to the chunk of line 83 in practice lecture 12. Use the Brier score to do the selection. Run the code and hit "stop" when you begin to the see the Brier score degrade appreciably.

```{r}
p_plus_one = ncol(Xmm_train)
predictor_by_iteration = c() #keep a growing list of predictors by iteration
in_sample_brier_by_iteration = c() #keep a growing list of se's by iteration
oos_brier_by_iteration = c() #keep a growing list of se's by iteration
i = 1

repeat {

  #get all predictors left to try
  all_brier = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (!(j_try %in% predictor_by_iteration)){
      Xmm_sub = Xmm_train[, c(predictor_by_iteration, j_try), drop = FALSE]
      logistic_mod = glm(ytrain ~ ., data.frame(Xmm_sub), family = "binomial")
      phatTrain = predict(logistic_mod, data.frame(Xmm_sub), type = 'response')
      all_brier[j_try] = mean(-(ytrain - phatTrain)^2) 
    }
  }
  j_star = which.min(all_brier)
  predictor_by_iteration = c(predictor_by_iteration, j_star)
  in_sample_brier_by_iteration = c(in_sample_brier_by_iteration, all_brier[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, predictor_by_iteration, drop = FALSE]
  logistic_mod = glm(ytrain ~ ., data.frame(Xmm_sub), family = "binomial")
  phatTrain = predict(logistic_mod, data.frame(Xmm_sub), type = 'response')
  all_brier[j_try] = mean(-(ytrain - phatTrain)^2)
  
  phat_select = predict(logistic_mod, Xmm_select[, predictor_by_iteration, drop = FALSE], type = 'response')
  
  
  oos_brier = mean(-(yselect - phat_select)^2) 
  oos_brier_by_iteration = c(oos_brier_by_iteration, oos_brier)
  
  cat("i = ", i, "in sample: brier = ", all_brier[j_star], "oos_brier", oos_brier, "\n   predictor added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  
  if (i > 5000 || i > p_plus_one){
    break #why??
  }
}
```

Plot the in-sample and oos (select set) Brier score by $p$. Does this look like what's expected?

```{r}
#TO-DO
```

Print out the coefficients of the model selection procedure's guess as to the locally optimal probability estimation model and interpret the five largest (in abolute value) coefficients. Do the signs make sense on these coefficients?

```{r}
#TO-DO
```

Use this locally optimal probability estimation model to make predictions in all three data sets: train, select test. Compare to the Brier scores across all three sets. Is this expected?

```{r}
#TO-DO
```

Plot the probability predictions in the test set by `y`. Does this plot look good?

```{r}
#TO-DO
```

Calculate misclassification error, sensitivity (recall), specificity (true negative rate, TN / N), FDR, FOR for this model if you threshold at phat = 0.5. Interpret these metrics.

```{r}
#TO-DO
```

Now, consider an asymmetric costs scenario. Let's say you're trying to sell people luxury products and want to advertise with only high-salaried individuals. Since your advertising is expensive, you want to not waste money on people who do not make a high salary. Thus your cost of predicting >50K when it truly is <=50K, i.e. a false positive (FP), is higher than predicting <=50K when the person truly makes >50K, i.e. a false negative (FN). Set the cost of FP to 3x more than the cost of FN. Use a grid of 0.001 to step through thresholds for the locally optimal probability estimation model (source the function from practice lecture 15). Do this in the selection dataset.

```{r}
#TO-DO
```

Plot an ROC curve for the selection dataset.

```{r}
#TO-DO
```

Calculate AUC and interpret.

```{r}
#TO-DO
```

Plot a DET curve for the selection dataset.

```{r}
#TO-DO
```

Calculate total cost for each classification model defined by each threshold.

```{r}
#TO-DO
```

Find the probability estimate threshold for the locally optimal asymmetric cost model for your FP and FN costs. Use this optimal probability estimate threshold and classify the test set. Print out its confusion matrix in the test set and calculate average cost per future observation, future FDR and future FOR and interpret these metrics in the context of this scenario. Is this model successful in internalizing your asymmetric costs?

```{r}
#TO-DO
```

Throughout the next part of this assignment you can use either the `tidyverse` package suite or `data.table` to answer but not base R. You can mix `data.table` with `magrittr` piping if you wish but don't go back and forth between `tbl_df`'s and `data.table` objects.

```{r}
pacman::p_load(tidyverse, magrittr, data.table)
```

We will be using the `storms` dataset from the `dplyr` package. Filter this dataset on all storms that have no missing measurements for the two diameter variables, "ts_diameter" and "hu_diameter".

```{r}
#TO-DO
```

From this subset, create a data frame that only has storm, observation period number (i.e., 1, 2, ..., T) and the "ts_diameter" and "hu_diameter" metrics.

```{r}
#TO-DO
```

Create a data frame in long format with columns "diameter" for the measurement and "diameter_type" which will be categorical taking on the values "hu" or "ts".

```{r}
#TO-DO
```

Using this long-formatted data frame, use a line plot to illustrate both "ts_diameter" and "hu_diameter" metrics by observation period for four random storms using a 2x2 faceting. The two diameters should appear in two different colors and there should be an appropriate legend.

```{r}
#TO-DO
```

