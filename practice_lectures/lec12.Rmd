---
title: "Practice Lecture 12 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 14, 2020"
---

# (3) Stepwise Model Construction

There are many types of such stepwise models. Here we will look at Forward Stepwise Linear models. "Forward" meaning we start with a low complexity model and end with a high complexity model, "Stepwise" meaning we do so iteratively which each step consisting of one additional degree of freedom i.e. one incremental increase in complexity and "Linear" meaning that the model is linear. By default we use OLS.

We will be using the diamonds data again as an example. Let's make sure we have unordered factors to avoid issues later:

```{r}
pacman::p_load(tidyverse, magrittr)
diamonds %<>%
  mutate(cut = factor(as.character(cut)), color = factor(as.character(color)), clarity = factor(as.character(clarity)))
```

What we're doing will be highly computational, so let's take a random sample of the dimaonds in $\mathbb{D}$:

```{r}
Nsamp = 1300
train_indices = sample(1 : nrow(diamonds), Nsamp)
diamonds_train = diamonds[train_indices, ]
```

Let's built a model with all second-order interactions e.g. all things that look like depth x table x clarity or depth^2 x color or depth^3.

```{r}
mod = lm(price ~ . * . * ., diamonds_train)
```

How many variables is this? And what does it look like?

```{r}
length(coef(mod))
coef(mod)[1000 : 1100]
```

For features that are non-binary, it's p_non_binary^3 features. Binary features are more complicated because its each level in feature A times each level in feature B. There are no squared or cube terms for binary features (since they're all the same i.e. ${0,1}^d = {0,1}$).

Remember we lkely overfit just using first order interactions? We'll certainly overfit using first-order interactions AND second order interactions

```{r}
summary(mod)$r.squared
sd(summary(mod)$residuals)
```

Is that believable? Well... let's try it on the another 10,000 we didn't see...

```{r}
test_indices = sample(setdiff(1 : nrow(diamonds), train_indices), Nsamp)
diamonds_test = diamonds[test_indices, ]
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
```

VERY negative oos $R^2$ --- why? What should that say about the relationship between $s_e$ and $s_y$?

```{r}
sd(y_test)
sd(e_test) / sd(y_test)
```

This is not only "overfitting"; it is an absolute trainwreck! This means you can do better using the null model (average of y) instead of this model.

So let us employ stepwise to get a "good" model. We need our basis predictors to start with. How about the linear components of `. * . * .` --- there's nothing intrinsically wrong with that - it's probably a good basis for $f(x)$. Let's create the model matrix for both train and test:

```{r}
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)
p_plus_one

Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
```

Now let's go through one by one and add the best one based on $s_e$ gain i.e. the best new dimension to add to project the most of the vector $y$ as possible onto the column space.

```{r}
predictor_by_iteration = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1

repeat {

  #get all predictors left to try
  all_ses = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (!(j_try %in% predictor_by_iteration)){
      Xmm_sub = Xmm_train[, c(predictor_by_iteration, j_try), drop = FALSE]
      all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm! 
    }
  }
  j_star = which.min(all_ses)
  predictor_by_iteration = c(predictor_by_iteration, j_star)
  in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, predictor_by_iteration, drop = FALSE]
  mod = lm.fit(Xmm_sub, y_train)
  y_hat_test = Xmm_test[, predictor_by_iteration, drop = FALSE] %*% mod$coefficients
  oos_se = sd(y_test - y_hat_test)
  oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
  
  cat("i = ", i, "in sample: se = ", all_ses[j_star], "oos_se", oos_se, "\n   predictor added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  predictor_by_iteration
  
  if (i > Nsamp || i > p_plus_one){
    break #why??
  }
  
}
```

Now let's look at our complexity curve:

```{r}
simulation_results = data.frame(
  iteration = 1 : length(in_sample_ses_by_iteration),
  in_sample_ses_by_iteration = in_sample_ses_by_iteration,
  oos_ses_by_iteration = oos_ses_by_iteration
)

pacman::p_load(latex2exp)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") + 
  ylab(TeX("$s_e$"))
```


We can kind of see what the optimal model is above. If we want an exact procedure, we'd probably fit a separate smoothing regression to the oos results and analytically find the arg-minimum, $j^*$. That number will then be fed into the model matrix to create the right feature set and the final model will be produced with all the data. Or we can just stop as soon as oos error goes up. You can also obviously do CV within each iterations to stabilize this further (lab exercise).

Can we honestly assess future performance now? No... why? Our test set was really our select set and we don't have a third test set (lab exercise). Inner and outer folding can be done too as we discussed.

# Regression Trees

Let's take a look at the simulated sine curve data.

```{r}
pacman::p_load(tidyverse, magrittr)
N = 500
x_max = 10
x = runif(N, 0, x_max)
y = sin(x) + rnorm(N, 0, 0.3)
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd=0.6) 
```


Let's fit a regression tree. We will use the development package "YARF" which I've been hacking on now for a few years. The package internals are written in Java. So we first need to ensure that Java and R speak to each other through proper configuration of the `rJava` package. You need to have a full JDK of Java installed on your computer and have its binary executables in the proper path. This demo will be in Java JDK 8 since I haven't tested `YARF` in the more modern Java JDK's yet.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(rJava)
#if that doesn't work, use:
# install.packages("rJava", type = "source")
# library(rJava)
.jinit() #this initializes the JVM in the background and if this runs with no issues nor output, you probably have rJava installed and connected to the JDK properly.
```

Just like the whole `Rcpp` demo, we can do a whole demo with `rJava`, but we won't. Here's just an example of creating a Java object and running a method on it:

```{r}
java_double = .jnew("java/lang/Double", 3.1415)
java_double
class(java_double)
.jclass(java_double)
#call an instance method 
.jcall(java_double, "I", "intValue")
#call a static method
J("java/lang/String", "valueOf", java_double)
J("java/lang/String", "valueOf", x) #some sort of alphanumeric code for the pointer address
```

A note on rJava vs Rcpp. 

* If you're doing quick and dirty fast functions for loops and recursion, do it in Rcpp since there is lower overhead of programming. 
* If you are programming more full-featured software, go with rJava. 
* Also, if you need full-featured parallelized execution and threading control e.g. thread pooling and the ease of debugging, my personal opinion is that rJava is easier to get working with less dependencies. My experience is that the Rcpp threading libraries just aren't there yet and neither is openMP directives within Rcpp. 
* Further, the JVM is fully asynchronous which means it runs completely independently of R. What this means is that you can execute something in Java, Java can "thread it off" and return you to the R prompt with a pointer to the object that houses its execution. You can then query the object. We will see dems of this.

Now we install the package from github including its dependency and load it:

```{r}
# pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
# pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev")
pacman::p_load(YARF)
```


Then run the fit:

```{r}
tree_mod = YARFCART(data.frame(x = x), y)
```

How "big" is this tree model?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

What does $g(x)$ look like?

```{r}
Nres = 1000
x_predict = data.frame(x = seq(0, x_max, length.out = Nres))
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Obviously overfit - but not that bad... let's try lowering the complexity by stopping the tree construction at a higher node size.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 500)
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Less overfitting now but now it's clearly underfit!

Now let's look at some real data. We load the Boston Housing data:

```{r}
pacman::p_load(MASS)
data(Boston)
?Boston
```

Now we split the data:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(Boston), round((1 - test_prop) * nrow(Boston)))
Boston_train = Boston[train_indices, ]
y_train = Boston_train$medv
X_train = Boston_train
X_train$medv = NULL
```

And fit a tree model:

```{r}
tree_mod = YARFCART(X_train, y_train)
```

What does the in-sample fit look like?

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

Recall the linear model:

```{r}
linear_mod = lm(medv ~ ., Boston_train)
summary(linear_mod)$sigma
summary(linear_mod)$r.squared
```

The tree seems to win in-sample. Why? 

Is this a "fair" comparison?

Before we address this, let's illustrate the tree. 

```{r}
illustrate_trees(tree_mod, max_depth = 3, open_file = TRUE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's make the comparison fair by seeing what happens oos.

```{r}
test_indices = setdiff(1 : nrow(Boston), train_indices)
Boston_test = Boston[test_indices, ]
y_test = Boston_test$medv
X_test = Boston_test
X_test$medv = NULL
```

For the tree:

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

For the linear model:

```{r}
y_hat_test_linear = predict(linear_mod, Boston_test)
e = y_test - y_hat_test_linear
sd(e)
1 - sd(e) / sd(y_test)
```

The take-home message here is that the tree beats the linear model in future predictive performance but the only way to be truly convinced of this is to do the split over and over to get a sense of the average over the massive variability (like the previous demo) or to do CV to reduce the error of the estimate. 

Why does the regression tree beat the linear model? Let's see what's going on in the tree.

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

About how many observations are in each leaf?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

That's a very flexible model.

Let's see overfitting in action. Let's set nodesize to be one.

```{r}
tree_mod = YARFCART(X_train, y_train, nodesize = 1)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why is it not exactly 1 on average? I think it's because...

Regardless of this point, this model is essentially giving each obsevation it's own y-hat, it's own personal guess which will be its own personal y. Just like linear modeling when $n = p + 1$ and nearest neighbors when $K = 1$. Let's see how bad the overfitting is:

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

This is strange. The overfit model still beats the linear model. It must be that the linear model is so *underfit* that it's beaten by a very *overfit* model. We can run this many times to see the variability...

```{r}
rm(list = ls())
```



# Classification Trees

Let's get the cancer biopsy data:

```{r}
biopsy = na.omit(MASS::biopsy)
biopsy$ID = NULL
str(biopsy)
?biopsy
```

Let's do a training-test split to keep things honest:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a tree:

```{r}
tree_mod = YARFCART(X_train, y_train)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(biopsy_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why would the average observations per node be larger than the nodesize which is 1?

```{r}
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

How are we doing in-sample?

```{r}
y_hat_train = predict(tree_mod, biopsy_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, biopsy_test)
mean(y_test != y_hat_test)
```

Still pretty good!

Now let's take a look at the linear SVM model.

```{r}
pacman::p_load(e1071)
svm_model = svm(X_train, y_train, kernel = "linear")
```

A couple of points:

* Reached max iterations to minimize the hinge loss. Seems like there are computational issues here.
* Note that we are relying on the $\lambda$ hyperparameter value for the hinge loss. On the homework, you will answer the question we never answered: how should the value of the hyperparameter be chosen?

Regardless, how did it do in-sample?

```{r}
y_hat_train = predict(svm_model, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, X_test)
mean(y_test != y_hat_test)
```

Maybe the model truly was linearly separable? Meaning, you don't get any added benefit from the tree if there are no interactions or non-linearities. Let's try a harder dataset. First, get a bunch of datasets from the UCI repository:

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
test_size = 2000
train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(adult_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

In-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

Looks like we overfit quite a bit! That's what nodesize of 1 does! Why is it the default? People found that even though it overfits, you still get good performance.

Let's see how the linear SVM does:

```{r}
svm_model = svm(model.matrix(~ ., X_train), y_train, kernel = "linear")
```

In-sample?

```{r}
y_hat_train = predict(svm_model, model.matrix(~ ., X_train))
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, model.matrix(~ ., X_test))
mean(y_test != y_hat_test)
```

It seems (at least when I ran it at home), the linear SVM does much worse. Likely there are a lot of interactions in this dataset that the linear SVM must ignore because it's $\mathcal{H}$ candidate set is so limited!

Note: SVM train error is approximtely = SVM test error? Why? 

That's a usual scenario during underfitting in high n situations. There is no estimation error - only misspecification error and error due to ignorance.


# Bias-Variance Decomposition of Generalization Error


Let's try to fit a quadratic $f$ with a linear model and examine bias-variance tradeoff.

```{r}
rm(list = ls())
xmin = 0
xmax = 5
n_train = 20
n_test = 1000
sigsq = 1
resolution = 10000

Nsim = 1000

training_gs = matrix(NA, nrow = Nsim, ncol = 2)
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)
x_trains = matrix(NA, nrow = Nsim, ncol = n_train)
y_trains = matrix(NA, nrow = Nsim, ncol = n_train)
for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sqrt(sigsq)) #mean zero, variance sigsq always (not dependent on value of x)
  y_train = x_train^2 + delta_train
  x_trains[nsim, ] = x_train
  y_trains[nsim, ] = y_train
  
  
  #fit a model g | x's, delta's and save it
  g_model = lm(y_train ~ ., data.frame(x = x_train))
  training_gs[nsim, ] = coef(g_model)
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigsq) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = x_test^2 + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Graph a few:

```{r}
pacman::p_load(ggplot2)
ggplot() + 
  geom_point(aes(x, y), data = data.frame(x = x_trains[1, ], y = y_trains[1, ]), col = "blue") +
  geom_point(aes(x, y), data = data.frame(x = x_trains[2, ], y = y_trains[2, ]), col = "darkgreen") +
  geom_point(aes(x, y), data = data.frame(x = x_trains[3, ], y = y_trains[3, ]), col = "red")
```

The blue dataset is one possible $\mathbb{D}$, the green dataset is another possible $\mathbb{D}$ and the red dataset is another possible $\mathbb{D}$. This illustrated "dataset-dataset variability".

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

This should be irreducible error plus bias-squared plus variance.

Let's visualize the bias

```{r}
g_average = colMeans(training_gs)
f = function(x){x^2}
x = seq(xmin, xmax, length.out = resolution)
ggplot(data.frame(x = x, f = f(x)), aes(x, f)) + 
  geom_line(col = "darkgreen") + 
  geom_abline(intercept = g_average[1], slope = g_average[2], col = "red")
```

What is the average bias of $g$?

```{r}
x = seq(xmin, xmax, length.out = resolution)
g_avg_x = g_average[1] + g_average[2] * x
f_x = x^2
biases = f_x - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

What is the variance? Let's look at all lines:

```{r}
plot_obj = ggplot() + 
  xlim(xmin, xmax) + ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 100)){ #otherwise takes too long
  plot_obj = plot_obj + geom_abline(intercept = training_gs[nsim, 1], slope = training_gs[nsim, 2], col = "blue")
}

plot_obj +
  geom_abline(intercept = g_average[1], slope = g_average[2], col = "red", lwd = 2)
```

Now calculate this average variance:

```{r}
x = seq(xmin, xmax, length.out = resolution)

expe_g_x = g_average[1] + g_average[2] * x

var_x_s = array(NA, Nsim)
for (nsim in 1 : Nsim){
  g_x = training_gs[nsim, 1] + training_gs[nsim, 2] * x
  var_x_s[nsim] = mean((g_x - expe_g_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Now check the equivalence

```{r}
mse
sigsq
expe_bias_g_sq
expe_var_g
sigsq + expe_bias_g_sq + expe_var_g
```

This is not exactly equal due to numerical error.

Let's try the whole thing again using a quadratic regression!

```{r}
training_gs = matrix(NA, nrow = Nsim, ncol = 3)
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sqrt(sigsq)) #mean zero, variance sigsq always (not dependent on value of x)
  y_train = x_train^2 + delta_train
  
  #fit a model g | x's, delta's and save it
  g_model = lm(y_train ~ poly(x, 2, raw = TRUE), data.frame(x = x_train))
  training_gs[nsim, ] = coef(g_model)
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigsq) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = x_test^2 + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Much lower! Why? Bias went down. 

Let's visualize the bias

```{r}
g_average = colMeans(training_gs)
f = function(x){x^2}
x = seq(xmin, xmax, length.out = resolution)
ggplot(data.frame(x = x, f = f(x)), aes(x, f)) + 
  geom_line(col = "darkgreen") + 
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2}, col = "red")
```

Not much! What is the average bias of $g$?

```{r}
x = seq(xmin, xmax, length.out = resolution)
g_avg_x = g_average[1] + g_average[2] * x + g_average[3] * x^2
f_x = x^2
biases = f_x - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

What is the variance? Let's look at all lines:

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) + ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2), mapping = aes(x, y), col = "blue")
}

plot_obj +
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2}, col = "red", lwd = 2)
```

Now calculate this average variance:

```{r}
x = seq(xmin, xmax, length.out = resolution)

expe_g_x = g_average[1] + g_average[2] * x + g_average[3] * x^2

var_x_s = array(NA, Nsim)
for (nsim in 1 : Nsim){
  g_x = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2
  var_x_s[nsim] = mean((g_x - expe_g_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Now check the equivalence

```{r}
mse
sigsq
expe_bias_g_sq
expe_var_g
sigsq + expe_bias_g_sq + expe_var_g
```


Try it again with quintic polynomials!

```{r}
training_gs = matrix(NA, nrow = Nsim, ncol = 6)
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sqrt(sigsq)) #mean zero, variance sigsq always (not dependent on value of x)
  y_train = x_train^2 + delta_train
  
  #fit a model g | x's, delta's and save it
  g_model = lm(y_train ~ poly(x, 5, raw = TRUE), data.frame(x = x_train))
  training_gs[nsim, ] = coef(g_model)
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sigsq) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = x_test^2 + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Much higher! Why? Variance went up!

Let's visualize the bias

```{r}
g_average = colMeans(training_gs)
f = function(x){x^2}
x = seq(xmin, xmax, length.out = resolution)
ggplot(data.frame(x = x, f = f(x)), aes(x, f)) + 
  geom_line(col = "darkgreen") + 
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5}, col = "red")
```

Not much! Now acutllay compute the average bias squared of $g$:

```{r}
x = seq(xmin, xmax, length.out = resolution)
g_avg_x = g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5
f_x = x^2
biases = f_x - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

This appears to have increased over last time ... but it's only because we're not running the regression infinite times. Remember this "expectation" is only an average.

What is the variance? Let's look at all lines:

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) + ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2 + training_gs[nsim, 4] * x^3 + training_gs[nsim, 5] * x^4 + training_gs[nsim, 6] * x^5), mapping = aes(x, y), col = "blue")
}

plot_obj +
  stat_function(fun = function(x){g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5}, col = "red", lwd = 2)
```

It looks awful!!!

Now actually compute the average variance numerically:

```{r}
x = seq(xmin, xmax, length.out = resolution)

expe_g_x = g_average[1] + g_average[2] * x + g_average[3] * x^2 + g_average[4] * x^3 + g_average[5] * x^4 + g_average[6] * x^5

var_x_s = array(NA, Nsim)
for (nsim in 1 : Nsim){
  g_x = training_gs[nsim, 1] + training_gs[nsim, 2] * x + training_gs[nsim, 3] * x^2 + training_gs[nsim, 4] * x^3 + training_gs[nsim, 5] * x^4 + training_gs[nsim, 6] * x^5
  var_x_s[nsim] = mean((g_x - expe_g_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Any more complexity than you need allows for overfitting!

Now check the equivalence

```{r}
mse
sigsq
expe_bias_g_sq
expe_var_g
sigsq + expe_bias_g_sq + expe_var_g
```

# Bias - Variance Decomposition of MSE in Regression Trees

Let's return to the simulated sine curve data which we used to introduce regression trees.

```{r}
rm(list = ls())
n_train = 100
n_test = 500
xmin = 0
xmax = 10
sigsq = 0.09
Nsim = 250
resolution = 1000
```

And load the tree package:

```{r}
options(java.parameters = "-Xmx4000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
pacman::p_load(YARF, tidyverse)
```

Now let's generate lots of different datasets and fit many tree models. Note there's a new argument `calculate_oob_error = FALSE`. This is here for speed only. Ignore this for now as we will go over what this means later in detail.

```{r}
training_gs = list() #storing entire objects - need a hash
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sqrt(sigsq))
  y_train = sin(x_train) + delta_train
  # ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd=0.6) 
  
  #fit a model g | x's, delta's and save it
  g_model = YARFCART(data.frame(x = x_train), y_train, calculate_oob_error = FALSE)
  training_gs[[nsim]] = g_model
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sqrt(sigsq)) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = sin(x_test) + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Let's look at the last data set to remind ourselves of the problem setting:

```{r}
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```


What does the storage of all the models look like?

```{r}
head(training_gs)
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Let's visualize the bias

```{r}
#generate x and the truth
x = seq(xmin, xmax, length.out = resolution)
f_x = sin(x)

#now estimate the expectation of g by averaging all the different models
g_avg_x = array(0, resolution)
for (nsim in 1 : Nsim){
  g_nsim = training_gs[[nsim]]
  g_avg_x = g_avg_x + predict(g_nsim, data.frame(x = x))
}
g_avg_x = g_avg_x / Nsim #average of all models

#now plot
ggplot(data.frame(x = x, f = f_x, expe_g = g_avg_x)) + 
  geom_line(aes(x, f), col = "darkgreen") + 
  geom_line(aes(x, expe_g), col = "red")
```

Not much! Now actually compute the average bias squared of $g$:

```{r}
biases = f_x - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

This is small - why??

It's because trees are so expressive and have such model complexity that they can nail almost any true $f$ function!

That means the MSE save the irreducible noise is coming from the variance. Let's look at one of the lines:

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) #+ ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 1)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = g_x), aes(x = x, y = y), col = "blue")
}

plot_obj +
  geom_line(data = data.frame(x = x, expe_g = g_avg_x), mapping = aes(x, expe_g), col = "red", lwd = 1.5)
```

It looks awful!!!

Now actually compute the average variance numerically:

```{r}
x = seq(xmin, xmax, length.out = resolution)

var_x_s = array(NA, min(Nsim, 50))
for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  var_x_s[nsim] = mean((g_x - g_avg_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Any more complexity than you need allows for overfitting!

Now check the equivalence

```{r}
mse
sigsq
expe_bias_g_sq
expe_var_g
sigsq + expe_bias_g_sq + expe_var_g
```

# Model Averaging via Bootstrap Aggregation ("Bagging")

```{r}
rm(list = ls())
n_train = 100
n_test = 500
xmin = 0
xmax = 10
sigsq = 0.09
num_trees = 250
resolution = 1000
```

And load the tree package:

```{r}
options(java.parameters = "-Xmx4000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
pacman::p_load(YARF)
```

Now let's generate one dataset (only ONE) 

```{r}
x_train = runif(n_train, xmin, xmax)
delta_train = rnorm(n_train, 0, sqrt(sigsq))
y_train = sin(x_train) + delta_train
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```

Spend a moment to appreciate that this is all we're going to have. There is a lot of irreducible noise that is going to prevent us from finding the sine curve.

Now we create many bootstrap samples to create similiar but different datasets and fit many tree models:

```{r}
bootstrapped_gs = list() #storing entire objects - need a hash

for (t in 1 : num_trees){
  #fit a model g | x's, delta's and save it
  bootstrap_indices_t = sample(1 : n_train, replace = TRUE)
  g_model = YARFCART(data.frame(x = x_train[bootstrap_indices_t]), y_train[bootstrap_indices_t], calculate_oob_error = FALSE)
  bootstrapped_gs[[t]] = g_model
}
```

Now let's aggregate all models constructed with bootstrap samples together by averaging and see what it looks like relative to real $f$:

```{r}
#generate x and the truth
x = seq(xmin, xmax, length.out = resolution)
f_x = sin(x)

#create the bagged model predictions
g_bagged = array(0, resolution)
for (t in 1 : num_trees){
  g_t = bootstrapped_gs[[t]]
  g_bagged = g_bagged + predict(g_t, data.frame(x = x))
}
g_bagged = g_bagged / num_trees #average of all models

#now plot
ggplot(data.frame(x = x, f = f_x, g_bagged = g_bagged)) + 
  geom_line(aes(x, f), col = "darkgreen") + 
  geom_line(aes(x, g_bagged), col = "blue")
```
  
That's pretty good considering the plot of the raw data from above.

Now let's see if this is truly better than one tree. The way to do this is generate lots of datasets and try to estimate mse (with bias and variance).

To do this, we employ a convenience method called `YARFBAG`. We first define a number of trees in the "bag":

```{r}
num_trees = 250 #called "M" in class
```

And then we run the same simulation as we did above with one tree.

```{r}
Nsim = 250
training_gs = list() #storing entire objects - need a hash
all_residuals = matrix(NA, nrow = Nsim, ncol = n_test)

for (nsim in 1 : Nsim){
  #simulate dataset $\mathbb{D}$
  x_train = runif(n_train, xmin, xmax)
  delta_train = rnorm(n_train, 0, sqrt(sigsq))
  y_train = sin(x_train) + delta_train
  # ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd=0.6) 
  
  #fit a model g | x's, delta's and save it
  g_model = YARFBAG(data.frame(x = x_train), y_train, num_trees = num_trees, calculate_oob_error = FALSE)
  training_gs[[nsim]] = g_model
  
  #generate oos dataset and save residuals on oos data
  x_test = runif(n_test, xmin, xmax)
  delta_test = rnorm(n_test, 0, sqrt(sigsq)) #mean zero, variance sigsq always (not dependent on value of x)
  y_test = sin(x_test) + delta_test
  y_hat_test = predict(g_model, data.frame(x = x_test))
  all_residuals[nsim, ] = y_test - y_hat_test
}
```

Let's look at the last data set to remind ourselves of the problem setting:

```{r}
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```


What does the storage of all the models look like?

```{r}
head(training_gs)
```

Take a look at the mse:

```{r}
mse = mean(c(all_residuals)^2)
mse
```

Let's visualize the bias

```{r}
#generate x and the truth
x = seq(xmin, xmax, length.out = resolution)
f_x = sin(x)

#now estimate the expectation of g by averaging all the different models
g_avg_x = array(0, resolution)
for (nsim in 1 : Nsim){
  g_nsim = training_gs[[nsim]]
  g_avg_x = g_avg_x + predict(g_nsim, data.frame(x = x))
}
g_avg_x = g_avg_x / Nsim #average of all models

#now plot
ggplot(data.frame(x = x, f = f_x, expe_g = g_avg_x)) + 
  geom_line(aes(x, f), col = "darkgreen") + 
  geom_line(aes(x, expe_g), col = "red")
```

Not much! If anything it should be *better* than the bias of a single tree!

Now actually compute the average bias squared of $g$:

```{r}
biases = f_x - g_avg_x
expe_bias_g_sq = mean(biases^2)
expe_bias_g_sq
```

Almost zilch.

Now... variance?

```{r}
plot_obj = ggplot(data.frame(x = x)) + 
  xlim(xmin, xmax) #+ ylim(xmin^2, xmax^2)

for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  plot_obj = plot_obj + geom_line(data = data.frame(x = x, y = g_x), aes(x = x, y = y), col = "blue")
}

plot_obj +
  geom_line(data = data.frame(x = x, expe_g = g_avg_x), mapping = aes(x, expe_g), col = "red", lwd = 1.5)
```

There is variance, but less than previously when only one tree was used.

Now actually compute the average variance numerically:

```{r}
x = seq(xmin, xmax, length.out = resolution)

var_x_s = array(NA, min(Nsim, 50))
for (nsim in 1 : min(Nsim, 50)){ #otherwise takes too long
  g_nsim = training_gs[[nsim]]
  g_x = predict(g_nsim, data.frame(x = x))
  var_x_s[nsim] = mean((g_x - g_avg_x)^2)
}

expe_var_g = mean(var_x_s)
expe_var_g
```

Any more complexity than you need allows for overfitting!

Now check the equivalence

```{r}
mse
sigsq
expe_bias_g_sq
expe_var_g
sigsq + expe_bias_g_sq + expe_var_g
```

We have a better algorithm!

# Bagged Trees vs. a Linear Model

First we'll load the packages and data:

```{r}
rm(list = ls())
options(java.parameters = "-Xmx8000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
# pacman::p_load(YARF, ggplot2)
library(YARF)
diamonds_sample = diamonds[sample(1 : nrow(diamonds), 2000), ]
boston = MASS::Boston
cancer = MASS::biopsy
cancer$ID = NULL
cancer = na.omit(cancer)
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
adult_sample = adult[sample(1 : nrow(adult), 2000), ]
```

Let's look at the boston housing data first

```{r}
prop_test = 0.1
test_indices = sample(1 : nrow(boston), round(prop_test * nrow(boston)))
boston_test = boston[test_indices, ]
y_test = boston_test$medv
X_test = boston_test
X_test$medv = NULL
train_indices = setdiff(1 : nrow(boston), test_indices)
boston_train = boston[train_indices, ]
y_train = boston_train$medv
X_train = boston_train
X_train$medv = NULL

mod_lin = lm(y_train ~ ., X_train)
y_hat_test_lin = predict(mod_lin, X_test)
s_e_lin = sd(y_test - y_hat_test_lin)
s_e_lin

num_trees = 500
mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
y_hat_test_bag = predict(mod_bag, X_test)
s_e_bag = sd(y_test - y_hat_test_bag)
s_e_bag

cat("oos standard error reduction:", (1 - s_e_bag / s_e_lin) * 100, "%\n")
```

How much room was there to improve?

```{r}
summary(mod_lin)$r.squared
```

Whole lot of room to improve! That extra 26% or so of unexplained variance is split between bias and irreducible error due to unknown information. The bagged trees can get rid of the bias and minimize variance while doing so. And it did a great job!

Now the diamonds data:

```{r}
n_train = 500

training_indices = sample(1 : nrow(diamonds), n_train)
diamonds_train = diamonds[training_indices, ]
y_train = diamonds_train$price
X_train = diamonds_train
X_train$price = NULL


test_indices = setdiff(1 : nrow(diamonds), training_indices)
diamonds_test = diamonds[test_indices, ]
y_test = diamonds_test$price
X_test = diamonds_test
X_test$price = NULL


mod_lin = lm(y_train ~ ., X_train)
y_hat_test_lin = predict(mod_lin, X_test)
s_e_lin = sd(y_test - y_hat_test_lin)
s_e_lin

mod_bag = YARFBAG(X_train, y_train, calculate_oob_error = FALSE)
y_hat_test_bag = predict(mod_bag, X_test)
s_e_bag = sd(y_test - y_hat_test_bag)
s_e_bag

cat("oos standard error reduction:", (1 - s_e_bag / s_e_lin) * 100, "%\n")
```

```{r}
summary(mod_lin)$r.squared
```

Not a whole lot of room to improve! That extra 8% of unexplained variance is split between bias and irreducible error due to unknown information. But it did the best it can. It is likely what's remaining is due to information we are not privy to.


# Validation in Bagging?

We are using the "bootstrap" to get the trees. Can we do model validation in the same step? 

The answer is yes. For every tree, there was a bootstrap sample of the training set used to build the tree. But there are observations in $\mathbb{D}$ that are not in the bootstrap sample! About 1/3 on average are left out i.e. "out of bag (oob)". Over many trees, there are different oob subsets than become the full data set. So you actually have validation in a way on the whole dataset kind of like K-fold cross validation. Supposedly this validation is similar to K=2 in terms of performance. It is what everyone seems to use. 

Let's load the data and packages from last class and plot the data:

```{r}
rm(list = ls())
n_train = 100
n_test = 500
xmin = 0
xmax = 10
sigsq = 0.09
num_trees = 500
x_train = runif(n_train, xmin, xmax)
delta_train = rnorm(n_train, 0, sqrt(sigsq))
y_train = sin(x_train) + delta_train
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```

Let's look at one bagged tree model and compute OOB errors after construction:

```{r}
bagged_tree_mod = YARFBAG(data.frame(x = x_train), y_train, num_trees = num_trees) #TRUE is the default! Why? It's something you want to know!
bagged_tree_mod
```

How did this work? Let's look at the oob sets:

```{r}
cat("bootstrap indices:")
sort(bagged_tree_mod$bootstrap_indices[[1]])
cat("oob:")
sort(setdiff(1 : n_train, bagged_tree_mod$bootstrap_indices[[1]]))
cat("bootstrap indices:")
bagged_tree_mod$bootstrap_indices[[2]]
cat("oob:")
setdiff(1 : n_train, bagged_tree_mod$bootstrap_indices[[2]])
```

It took predictions on each tree on the oob set, averaged by observation across trees and then averaged across observation averages.


# Random Forests


What do we have now? We have model selection is done within training. And training and validation are done in a single step! No more costly K-fold CV with 3 splits!

But can it get any better? YES. As you saw, the variance terms can be shrunk further the more decorrelated the trees become. We do this now by introducing randomness into the splits by choosing only a subset of the features to split on randomly. The trees are then grown as normal. Then the we model average many trees via bagging. And that's random forests!

Quick demo with the boston housing data and oob validation:

```{r}
rm(list = ls())
boston = MASS::Boston
y = boston$medv
X = boston
X$medv = NULL

mod_bag = YARFBAG(X, y, num_trees = 500)
mod_bag
mod_rf = YARF(X, y, num_trees = 500)
mod_rf
```

Gain in decorrelation?

```{r}
cat("gain: ", (mod_rf$pseudo_rsq_oob - mod_bag$pseudo_rsq_oob) / mod_bag$pseudo_rsq_oob * 100, "%\n")
```

For this example, not much. How about on the diamonds dataset?

```{r}
diamonds = ggplot2::diamonds

rm(list = ls())
num_trees = 500
n_train = 500

training_indices = sample(1 : nrow(diamonds), n_train)
diamonds_train = diamonds[training_indices, ]
y_train = diamonds_train$price
X_train = diamonds_train
X_train$price = NULL

mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_rf = YARF(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)

test_indices = setdiff(1 : nrow(diamonds), training_indices)
diamonds_test = diamonds[test_indices, ]
y_test = diamonds_test$price
X_test = diamonds_test
X_test$price = NULL

mse_bag = sum((y_test = predict(mod_bag, X_test))^2) / nrow(X_test)
mse_rf = sum((y_test = predict(mod_rf, X_test))^2) / nrow(X_test)
cat("gain: ", (mse_bag - mse_rf) / mse_bag * 100, "%\n")
```

Also, not much, but real.

