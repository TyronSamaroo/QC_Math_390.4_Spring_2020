---
title: "Practice Lecture 13 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 17, 2020"
---

# Regression Trees with Real Data

Now let's look at a regression tree model predicting medv in the Boston Housing data. We first load the data and do a training-test split:

```{r}
pacman::p_load(MASS)
data(Boston)
test_prop = 0.1
train_indices = sample(1 : nrow(Boston), round((1 - test_prop) * nrow(Boston)))
Boston_train = Boston[train_indices, ]
y_train = Boston_train$medv
X_train = Boston_train
X_train$medv = NULL
n_train = nrow(X_train)
```

And fit a tree model. The default hyperparameter, the node size is $N_0 = 5$.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
tree_mod = YARFCART(X_train, y_train, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
```

What does the in-sample fit look like?

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

Recall the linear model:

```{r}
linear_mod = lm(medv ~ ., Boston_train)
summary(linear_mod)$sigma
summary(linear_mod)$r.squared
```

The tree seems to win in-sample. Why? 

Is this a "fair" comparison?

Before we address this, let's illustrate the tree. 

```{r}
illustrate_trees(tree_mod, max_depth = 3, open_file = TRUE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's make the comparison fair by seeing what happens oos.

```{r}
test_indices = setdiff(1 : nrow(Boston), train_indices)
Boston_test = Boston[test_indices, ]
y_test = Boston_test$medv
X_test = Boston_test
X_test$medv = NULL
```

For the tree:

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

For the linear model:

```{r}
y_hat_test_linear = predict(linear_mod, Boston_test)
e = y_test - y_hat_test_linear
sd(e)
1 - sd(e) / sd(y_test)
```

The take-home message here is that the tree beats the linear model in future predictive performance but the only way to be truly convinced of this is to do the split over and over to get a sense of the average over the massive variability (like the previous demo) or to do CV to reduce the error of the estimate. 

Why does the regression tree beat the linear model? Let's see what's going on in the tree.

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

About how many observations are in each leaf?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

That's a very flexible model.

Let's see overfitting in action. Let's set nodesize to be one.

```{r}
tree_mod = YARFCART(X_train, y_train, nodesize = 1, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why is it not exactly 1 on average? I think it's because...

```{r}
data.table::uniqueN(y_train)
length(y_train)
```

Regardless of this point, this model is essentially giving each observation it's own y-hat, it's own personal guess which will be its own personal y. Just like linear modeling when $n = p + 1$ and nearest neighbors when $K = 1$. Let's see how bad the overfitting is:

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

This is the expected behavior in perfect fitting.

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

Amazing it doesn't get clobbered completely! And its results are on-par with the non-overfit linear model. Trees are truly incredible and I still don't understand everything about them.

```{r}
rm(list = ls())
```

# Classification Trees

Let's get the cancer biopsy data:

```{r}
pacman::p_load(tidyverse, magrittr)
data(biopsy, package = "MASS")
biopsy %<>% na.omit %>% select(-ID)
colnames(biopsy) = c(
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's do a training-test split to keep things honest:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
n_train = nrow(X_train)
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a tree:

```{r}
tree_mod = YARFCART(X_train, y_train, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(biopsy_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why would the average observations per node be larger than the nodesize which is 1?

```{r}
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

How are we doing in-sample?

```{r}
y_hat_train = predict(tree_mod, biopsy_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, biopsy_test)
mean(y_test != y_hat_test)
```

Still pretty good!

Now let's take a look at the linear SVM model.

```{r}
pacman::p_load(e1071)
svm_model = svm(X_train, y_train, kernel = "linear")
```

A couple of points:

* Reached max iterations to minimize the hinge loss. Seems like there are computational issues here.
* Note that we are relying on the $\lambda$ hyperparameter value for the hinge loss. On the homework, you will answer the question we never answered: how should the value of the hyperparameter be chosen?

Regardless, how did it do in-sample?

```{r}
y_hat_train = predict(svm_model, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, X_test)
mean(y_test != y_hat_test)
```

Maybe the model truly was linearly separable? Meaning, you don't get any added benefit from the tree if there are no interactions or non-linearities. Let's try a harder dataset. First, get a bunch of datasets from the UCI repository:

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
test_size = 2000
train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
n_train = nrow(X_train)
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(adult_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

In-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

The warning was legit this time. What's it saying?

Looks like we overfit quite a bit! That's what nodesize of 1 does! Why is it the default? People found that even though it overfits, you still get good performance (as we've seen even with regression). I doubt people still use CART in production these days since this issue was fixed with bagging (we will get to this soon).

Let's see how the linear SVM does. Warning: this takes a while to compute:

```{r}
svm_model = svm(model.matrix(~ ., X_train), y_train, kernel = "linear")
```

In-sample?

```{r}
y_hat_train = predict(svm_model, model.matrix(~ ., X_train))
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, model.matrix(~ ., X_test))
mean(y_test != y_hat_test)
```

It seems (at least when I ran it at home), the linear SVM does much worse. Likely there are a lot of interactions in this dataset that the linear SVM must ignore because it's $\mathcal{H}$ candidate set is so limited!

Note: SVM train error is approximtely = SVM test error? Why? 

That's a usual scenario during underfitting in high n situations. There is no estimation error - only misspecification error and error due to ignorance. And those are the same among the training and test set.

# Wide and Long Dataframe Formats

We will demonstrate a new concept on the dataset from lab #8: storms.

```{r}
pacman::p_load(data.table, tidyverse, magrittr)
summary(storms)
head(storms)
```

Let's first create a few variables that are of interest:

```{r}
storms %<>% 
  mutate(wind_pct_avg = wind / mean(wind, na.rm = TRUE) * 100) %>%
  mutate(pressure_pct_avg = wind / mean(pressure, na.rm = TRUE) * 100) %>%
  mutate(ts_diameter_pct_avg = wind / mean(ts_diameter, na.rm = TRUE) * 100) %>%
  mutate(hu_diameter_pct_avg = wind / mean(hu_diameter, na.rm = TRUE) * 100)
ggplot(storms) + 
  aes(wind_pct_avg) + 
  geom_histogram()
```

Now let's take a look at these four variables we created for a storm we all remember and create a time period variable. I'll also instantiate a data.table object for later:

```{r}
sandy_wide_tbl = storms %>% 
  filter(name == "Sandy") %>%
  select(wind_pct_avg, pressure_pct_avg, ts_diameter_pct_avg, hu_diameter_pct_avg) %>% #we only care about our variables
  mutate(period = 1 : n()) %>%
  select(period, everything()) #reorder
sandy_wide_dt = data.table(sandy_wide_tbl)
sandy_wide_dt
```

This is called a "repeated measures" dataset or a "time series" and it is one of the most common data frame types. Unfortunately, we didn't have enough classtime to do a unit on time series. It really deserves its own class!

Regardless, it would be nice to be able to visualize It would be nice to look at the four variables we just created by time period. We can do this below:

```{r}
ggplot(sandy_wide_tbl) + 
  aes(x = period) + 
  geom_line(aes(y = wind_pct_avg), col = "red") + 
  geom_line(aes(y = pressure_pct_avg), col = "green") + 
  geom_line(aes(y = ts_diameter_pct_avg), col = "blue") + 
  geom_line(aes(y = hu_diameter_pct_avg), col = "grey") +
  ylab("% over average")
```

Notice how that was a lot of lines of code which aren't so maintainable and we don't have a legend. Legends are built automatically in `ggplot2` when we set color to a variable. This means we somehow have to let the four variables we care about be there own categorical variable.

First note that the dataframe we have is in what's called "wide format" or "unstacked" meaning each row is an observation and the columns are its features. This is exactly the format of dataframe that we've been studying in this class. This is the format we humans prefer to read and it is the format for many important analyses and the format for modeling.

However, to get what we want above involves a "reshaping" our dataframe into another canonical form, one that is easier for machines to read, a format called "long format" or "narrow" or "stacked" which looks like this:

| Period      | Value       | variable     |
| ----------- | ----------- | -------------|
| 1           | 56.08       | wind_pct_avg |
| 2           | 65.43       | wind_pct_avg |
etc.

Sometimes this format is required for situations, so we should get used to "pivoting" between the two formats. 

We first go from wide to long. To do so, we identify the "id variables" which get their own row per category and the measurement variables which get their own entire subdataframe.

```{r}
sandy_long_tbl = pivot_longer(
  sandy_wide_tbl, 
  cols = -period, #measurement variables: all column except period and period is then the ID variable
  names_to = "metric", #default is "name"
  values_to = "val" #default is "value"
)
sandy_long_dt = melt(
  sandy_wide_dt,
  id.vars = "period",
  measure.vars = c("wind_pct_avg", "pressure_pct_avg", "ts_diameter_pct_avg", "hu_diameter_pct_avg"),
  variable.name = "metric",
  value.name = "val"
)
sandy_long_tbl
sandy_long_dt
```

Same output but note the difference in sorting: `tidyverse` sorts on the id variables first and `data.table` sorts on the measurements i.e. cbinding the subdataframes.

Now that it's in long format, the visualization code becomes very simple:

```{r}
ggplot(sandy_long_dt) +
  geom_line(aes(x = period, y = val, color = metric)) +
  ylab("% over average")
```

Now we go from long to wide:

```{r}
sandy_wide_tbl2 = pivot_wider(
  sandy_long_tbl,
  id_cols = period, 
  names_from = metric,
  values_from = val
)
sandy_wide_dt2 = dcast(
  sandy_long_dt,
  period ~ metric, #lhs is id and rhs is measurement variables
  value.var = "val" #the function can guess "val" has to be the cell values so it's not needed
)
sandy_wide_tbl2
sandy_wide_dt2
```

Who's faster?

```{r}
pacman::p_load(microbenchmark)
microbenchmark(
  wide_to_long_tidy = pivot_longer(
    sandy_wide_tbl, 
    cols = -period,
    names_to = "metric",
    values_to = "val"
  ),
  wide_to_long_dt = melt(
    sandy_wide_dt,
    id.vars = "period",
    measure.vars = c("wind_pct_avg", "pressure_pct_avg", "ts_diameter_pct_avg", "hu_diameter_pct_avg"),
    variable.name = "metric",
    value.name = "val"
  ),
  long_to_wide_tidy = pivot_wider(
    sandy_long_tbl,
    id_cols = period, 
    names_from = metric,
    values_from = val
  ),
  long_to_wide_dt = dcast(
    sandy_long_dt,
    period ~ metric,
    value.var = "val"
  ),
  times = 50
)
```

Looks like ``data.table::melt`` is 60x faster than tidyverse's pivot and ``data.tabe::dcast` is 2x faster than tidyverse's pivot.

# Joins

Another one of the core data munging skills is joining data frames together. In the real world, databases consist of multiple dataframes called "tables" and design matrices are built by gathering data from among many tables. To illustrate this, we load two datasets from the package `nycflights13`, one dataset about weather and one about airports:

```{r}
pacman::p_load(nycflights13, data.table, tidyverse, magrittr)
data(weather)
summary(weather)
data(airports)
summary(airports)
```

Note how the weather and airports datasets contain a common feature: name of airport. It is called `FAA` in airports and `origin` in weather.

First we rename the column in weather to match the column in airports:

```{r}
weather %<>% 
  rename(faa = origin)
```

We also pare down the datasets so we can see the joins more clearly:

```{r}
airports %<>% 
  select(faa, lat, lon)
weather %<>% 
  select(faa, time_hour, temp, humid, wind_speed, pressure, wind_gust)
head(airports)
head(weather)
airports_dt = data.table(airports)
weather_dt = data.table(weather)
```

Some features just aren't measured that often e.g. `wind_gust`.

Let's do some joins. First "left". This is likely the most common because it's usually how we conceptualize what we're doing in our heads.

```{r}
airports_and_weather = left_join(airports, weather, by = "faa")
airports_and_weather %>% sample_n(500)
airports_and_weather_dt = merge(airports_dt, weather_dt, by = "faa", all.x = TRUE)
airports_and_weather_dt = merge(airports_dt, weather_dt, all.x = TRUE) #note this works too since it knows faa is the only column in common but not recommended since specifying "by" is more clear
airports_and_weather_dt[sample(1 : .N, 500)]
```

Now "right"

```{r}
airports_and_weather = right_join(airports, weather, by = "faa")
airports_and_weather %>% sample_n(500)
airports_and_weather_dt = merge(airports_dt, weather_dt, by = "faa", all.y = TRUE)
airports_and_weather_dt = merge(airports_dt, weather_dt, all.y = TRUE)
airports_and_weather_dt[sample(1 : .N, 500)]
```


```{r}
airports_and_weather = inner_join(airports, weather, by = "faa")
airports_and_weather %>% sample_n(500)
airports_and_weather_dt = merge(airports_dt, weather_dt, by = "faa")
airports_and_weather_dt = merge(airports_dt, weather_dt)
airports_and_weather_dt[sample(1 : .N, 500)]
```

And full, keeping all the rows. We use a subset to show how this works:

```{r}
airports_without_EWR = airports %>%
  filter(faa != "EWR")
airports_without_EWR_dt = data.table(airports_without_EWR)
airports_without_EWR_and_weather = full_join(airports_without_EWR, weather, by = "faa")
airports_without_EWR_and_weather %>% sample_n(500)
airports_without_EWR_and_weather_dt = merge(airports_without_EWR_dt, weather_dt, by = "faa", all = TRUE)
airports_without_EWR_and_weather_dt = merge(airports_without_EWR_dt, weather_dt, all = TRUE)
airports_without_EWR_and_weather_dt[sample(.N, 500)]
```

There is also `semi_join` and `anti_join` that do the opposite of joining. In my experience, these use cases are limited.
