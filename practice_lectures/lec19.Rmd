---
title: "lec19.Rmd"
author: "Adam Kapelner"
date: "May 7, 2020"
output: html_document
---

# Ridge Regression

Let's take a look at the boston housing data and add many useless features.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(data.table, tidyverse, magrittr, YARF)
boston = MASS::Boston %>% data.table
```

Now add a whole bunch of extra features:

```{r}
p_extra = 1000

set.seed(1)
boston = cbind(boston, matrix(rnorm(nrow(boston) * p_extra), ncol = p_extra))
dim(boston)
```

Clearly $p + 1 > n$ so OLS will not work. Let's try ridge with $\lambda = 0.01$. Let's see the ridge estimate:

```{r}
X = cbind(1, as.matrix(boston[, !"medv"]))
y = boston[, medv]
lambda = 0.1
b_ridge = solve(t(X) %*% X + lambda * diag(ncol(X))) %*% t(X) %*% y
head(b_ridge, 30)
```

Clearly this works as an algorithm where OLS wouldn't. 

Note: I left this out of the demos and out of class... you should standardize your features before ridge otherwise features become unfairly squished or unfairly dominant relative to others. Each should have te same weight.

But let's see how it performs relative to OLS. To do so, we'll use the same setup but not add quite as many junk features so we can compare to OLS:


```{r}
boston = MASS::Boston %>% data.table
p_extra = 350

set.seed(1)
boston = cbind(boston, matrix(rnorm(nrow(boston) * p_extra), ncol = p_extra))
dim(boston)
```

Now we'll split into train-test so we can see which does better.

```{r}
prop_test = 0.2
test_indices = sample(1 : nrow(boston), round(prop_test * nrow(boston)))
boston_test = boston[test_indices, ]
y_test = boston_test$medv
X_test = cbind(1, as.matrix(boston_test[, !"medv"]))
train_indices = setdiff(1 : nrow(boston), test_indices)
boston_train = boston[train_indices, ]
y_train = boston_train$medv
X_train = cbind(1, as.matrix(boston_train[, !"medv"]))
```

Let's use a big lambda since we have intuition that most of the features are junk:

```{r}
lambda = 10
```

And we'll fit both models:

```{r}
b_ols = solve(t(X_train) %*% X_train) %*% t(X_train) %*% y_train
b_ridge = solve(t(X_train) %*% X_train + lambda * diag(ncol(X_train))) %*% t(X_train) %*% y_train
abs(b_ols) %>% head(30)
abs(b_ridge) %>% head(30)
```

And look at oos performance:

```{r}
y_hat_ols = X_test %*% b_ols
y_hat_ridge = X_test %*% b_ridge
rmse_ols = sd(y_test - y_hat_ols)
rmse_ridge = sd(y_test - y_hat_ridge)
rmse_ols
rmse_ridge
cat("ridge advantage over OLS:", round((rmse_ols - rmse_ridge) / rmse_ols * 100, 1), "%")
```

Why did it do better than OLS???? Because penalized regression is a good idea if you know many of your features are junk. But only if you know many of your features are junk. 


Of course by using CV, we can optimize the lambda value to give ridge even better performance. The package `glmnet` does that for us automatically:

```{r}
pacman::p_load(glmnet)
ridge_mod_optimal_lambda = cv.glmnet(X_train, y_train, alpha = 0, lambda = 10^seq(-3, 3, by = 0.1))
y_hat_optimal_ridge = predict(ridge_mod_optimal_lambda, X_test)
rmse_optimal_ridge = sd(y_test - y_hat_optimal_ridge)
rmse_optimal_ridge
cat("optimal lambda:", ridge_mod_optimal_lambda$lambda.min, "\n")
cat("optimal ridge advantage over OLS:", round((rmse_ols - rmse_optimal_ridge) / rmse_ols * 100, 1), "%\n")
```

Of course you can use `mlr` as well but `glmnet` is probably more optimized.

# Lasso

Let's do this same problem using the lasso. There is no closed form solution since the design matrix is not orthogonal (i.e. there's some multicollinearity), so we will use the numerical optimization found in the `glmnet` package. While we're at it, we might as well use CV to find the best lambda.

```{r}
lasso_mod_optimal_lambda = cv.glmnet(X_train, y_train, alpha = 1, lambda = 10^seq(-3, 3, by = 0.1))
y_hat_optimal_lasso = predict(lasso_mod_optimal_lambda, X_test)
rmse_optimal_lasso = sd(y_test - y_hat_optimal_lasso)
rmse_optimal_lasso
cat("optimal lambda:", lasso_mod_optimal_lambda$lambda.min, "\n")
cat("optimal lasso advantage over OLS:", round((rmse_ols - rmse_optimal_lasso) / rmse_ols * 100, 1), "%\n")
```

Wow - did better than ridge in predictive accuracy. Lambda values are completely not comparable since L1 and L2 penalties are categorically different. 

What do the estimates look like?

```{r}
head(coef(lasso_mod_optimal_lambda), 30)
```

That "." means 0 in a sparse matrix. We never studied these. But they are very memory efficient. Which ones are non-zero?

```{r}
b_lasso = coef(lasso_mod_optimal_lambda)[, 1]
b_lasso[b_lasso != 0]
```

Wow - it deleted all 364 variables except for 4: intercept, rm, ptratio and lstat!!!

If you remember in the regression tree, these were the most important (highest level splits).  That is killer variable selection!

The coefficient values are also approximately the OLS estimates:

```{r}
lm(medv ~ rm + ptratio + lstat, MASS::Boston) #i.e. a regression on the original data with no junk
```

It's amazing that it really deleted ALL the junk and left the most predictive variables of the original 13 features and the estimates of those four is pretty on target.


# Elastic Net

We can use `mlr` to CV over alpha, but here we can't. So let's let $\alpha = 0.5$ meaning "half lasso and half ridge" penalty:

```{r}
elastic_net_mod_optimal_lambda = cv.glmnet(X_train, y_train, alpha = 0.5, lambda = 10^seq(-3, 3, by = 0.1))
y_hat_optimal_elastic_net = predict(elastic_net_mod_optimal_lambda, X_test)
rmse_optimal_elastic_net = sd(y_test - y_hat_optimal_elastic_net)
rmse_optimal_elastic_net
cat("optimal elastic_net advantage over OLS:", round((rmse_ols - rmse_optimal_elastic_net) / rmse_ols * 100, 1), "%\n")
cat("optimal lambda:", elastic_net_mod_optimal_lambda$lambda.min, "\n")
```

Slightly better than lasso. I imagine if we optimized $\alpha$ we can do even better. Elastic nets can also give easy variable selection:

```{r}
head(coef(elastic_net_mod_optimal_lambda), 30)
```

Here we "found" one more variable. That makes sense - as alpha decreases, the ridge penalty becomes more pronounced and it's harder to shrink exactly to zero. Unsure about the $\alpha$ value that stops the hard shrinkage to zero. Good project to think about!

#RF with many features

How about RF?

```{r}
rf_mod = YARF(data.frame(X_train), y_train, num_trees = 500, calculate_oob_error = FALSE)
rmse_rf = sd(y_test - predict(rf_mod, data.frame(X_test)))

cat("RF advantage over OLS:", round((rmse_ols - rmse_rf) / rmse_ols * 100, 1), "%\n")
```

Takes a very long time to build - why? Amazingly, RF does very well. Why? How it able to not get confused by the junk features? It might be because the real features have a slight SSE edge. I think RF will do poorly if p > n. Maybe a lab exercise?

How about just the RF on the lasso-picked variables? We can delete the intercept since RF doesn't need it.

```{r}
variables_selected = names(b_lasso[b_lasso != 0])
variables_selected = variables_selected[-1]
X_train_sub = data.frame(X_train)[, variables_selected]
X_test_sub = data.frame(X_test)[, variables_selected]

rf_mod = YARF(X_train_sub, y_train, num_trees = 500, mtry = 2, calculate_oob_error = FALSE)
rmse_rf = sd(y_test - predict(rf_mod, X_test_sub))

cat("RF var selected advantage over OLS:", round((rmse_ols - rmse_rf) / rmse_ols * 100, 1), "%\n")
```

Why is that better than lasso? Because lasso is linear and in RF you get a bit of juice from the non-linearities and interactions. Why is it very slightly better than RF on the full data set? Because variable selection is a good "pre-step" to do sometimes. This is why in the real world there's usually a "pipeline" that cleans data, then variable selects, then fits model then validates.

# Missingness

Take a look at the weather dataset again:

```{r}
rm(list = ls())
pacman::p_load(nycflights13, tidyverse, magrittr, data.table, skimr)
data(weather)
skim(weather)
```

Do some cleanup. Note how `time_hour` is perfectly collinear with the year, month, day and hour columns. So let's drop `time_hour`. There is only one year, so drop that feature. Also, character variables now allowed.

```{r}
weather %<>%
  select(-c(time_hour, year)) %>%
  mutate(origin = as.factor(origin))
```

Imagine we were trying to predict `precip`. So let's section our dataset:

```{r}
y = weather$precip
X = weather %>% 
  select(-precip)
rm(weather)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = tbl_df(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
head(M)
skim(M)
```

Some of these missing indicators are collinear because they share all the rows they are missing on. Let's filter those out:

```{r}
M = tbl_df(t(unique(t(M))))
M %<>% 
  select_if(function(x){sum(x) > 0})
skim(M)
```

Now let's work on imputation. The missingness is not extreme except in the `wind_gust` variable. Let's drop that variable but retain the information saying it was missing:

```{r}
X %<>% 
  select(-wind_gust)
```

Without imputing and without using missingness as a predictor in its own right, let's see what we get with a basic linear model now:

```{r}
lin_mod_listwise_deletion = lm(y ~ ., cbind(X, M))
summary(lin_mod_listwise_deletion)
```

A measly 13.2%. Also note: year is collinear.

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 26,000! observations) so we will sample 2,000 for each of the trees and then average. That will be good enough.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(1000, ncol(X)))$ximp
```

Now we take our imputed dataset, combine it with our missingness indicators for a new design matrix.

```{r}
Ximp_and_missing_dummies = data.frame(cbind(Ximp, M))
linear_mod_impute_and_missing_dummies = lm(y ~ ., Ximp_and_missing_dummies)
summary(linear_mod_impute_and_missing_dummies)
```

Is this a better model?? Are they even comparable? How to compare them? I'm not sure... certainly an R^2 comparison is the wrong way to compare.

Note: this is just an illustration of best practice. It didn't necessarily have to "work".

It is hard to compare the two models since the first model was built with 23,000 observations and this was built with the full 26,000 observations. Those extra 3,000 are the most difficult to predict on. This is complicated...

How does RF do?

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
YARF(Ximp, y, num_trees = 500, n_max_per_tree = 1000) 
YARF(Ximp_and_missing_dummies, y, num_trees = 500, n_max_per_tree = 1000)
```


# Spurious Correlation

Take a look at the following real data:

```{r}
rm(list = ls())
pacman::p_load(tidyverse, magrittr, data.table)

spurious = data.frame(
  yearly_divorce_rate_maine_per_1000 = c(5,4.7,4.6,4.4,4.3,4.1,4.2,4.2,4.2,4.1),
  yearly_US_consumption_margarine_per_capita = c(8.2,7,6.5,5.3,5.2,4,4.6,4.5,4.2,3.7)
)

with(spurious, 
     cor(yearly_divorce_rate_maine_per_1000, yearly_US_consumption_margarine_per_capita))
```

And visually,

```{r}
ggplot(spurious, aes(x = yearly_divorce_rate_maine_per_1000, y = yearly_US_consumption_margarine_per_capita)) +
  geom_point() + geom_smooth()
```

How did this happen? 

I looked at many, many different datasets until I found something impressive!

Well, we can imagine doing the same thing. Let's look at a million datasets and find the dataset most correlated with the yearly consumption of margarine per capita:


```{r}
y = spurious$yearly_US_consumption_margarine_per_capita
n = length(y)

n_sim = 1e6
best_abs_corr = 0
best_random_xs = NULL
for (i in 1 : n_sim){
  x = rnorm(n)
  
  random_abs_corr = abs(cor(x, y))
  if (random_abs_corr > best_abs_corr){
    best_abs_corr = random_abs_corr
    best_random_xs = x
  }
}
spurious$best_random_xs = best_random_xs

best_abs_corr
```

And visually,

```{r}
ggplot(spurious, aes(x = best_random_xs, y = yearly_US_consumption_margarine_per_capita)) +
  geom_point() + geom_smooth() + ggtitle(paste("Spurious Correlation has |r| = ", round(best_abs_corr, 3)))
```

So what's the narrative here? If you look through a gajillion random features that have no causal connection with the phenomenon $y$, you will eventually find something that "clicks". Here are a whole bunch of them:

https://www.tylervigen.com/spurious-correlations

However, these will all vanish if you keep collecting data. Anything that is built upon falsehood will crumble!

