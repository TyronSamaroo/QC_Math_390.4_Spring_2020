---
title: "Practice Lecture 3 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 14, 2019"
---


## Libraries in R

So far we've only made use of "base R". This is the funcionality included from a vanilla installation of R. 

R has a huge worldwide community of contributors. Some contribute to newer version of base R, but most create open-source "R packages" or "libraries". Many libraries come preinstalled. For instance, the MASS library which stands for "Modern Applied Statistics with S" (a famous textbook of R). We can call a function from the MASS library via the following:

```{r}
MASS::as.fractions(0.99)
MASS::as.fractions(pi)
```

Note we cannot just

```{r}
as.fractions(pi)
```

We made use of the scope operator "::" to access a namespace beyond the usual "global namespace" which we've been used to. Parenthetically, you can use the ":::" to access the private / internal functions and variables. Anyone who understands object-oriented programming with public interfaces / APIs would cringe at this!!!

If we are using the MASS library a lot, using the scope operator may get annoying. So similar to the "with" command, we can call

```{r}
library(MASS)
```

which loads all public methods (aka "exported" functions) into the public namespace. 

Now, after the library invocation we can do the following and treat it as a normal function:

```{r}
as.fractions(pi)
```

Is this always a good idea? No... everytime you call `library` it "dirties" the namespace by putting all the functions there and rewriting over functions there previously. This is bad because you are more likely to get namespace conflicts. For instance. Let's say package `kapelner` had a weird `sample` function. This would be clear:

```{r}
v = rnorm(100)
kapelner::sample(v)
sample(v)
```

The first line is doing the special sample function and the second is using base R's sample. But if I do this:

```{r}
library(kapelner)
###10,000 lines of code in which you forget about the fact that the kapelner library is loaded
sample(v)
```

You may think you're getting base R sample function, but you're not and it's bug-city. You would have to do the following to be explicit:

```{r}
library(kapelner)
sample(v)
base::sample(v)
```

This is not a recommended thing to do. It's also not recommended for package developers to name functions the same as common base R functions. But this doesn't stop them!

Back to real packages... the content for the MASS package was sitting on the hard drive since it comes with R. But what if you want to use a package that does not come with R? We'll have to install the package just like pip for Python, Rubygems for Ruby, R has a package management system built in. For example, here's a useful package for time series / finance stuff:

```{r}
install.packages("tseries")
```

Note that it knew where to go online - it went to a CRAN mirror. CRAN is the official repository for R packages. Now that it's installed, step 2 is to load it into namespace so we can more seamlessly access its functionality.

```{r}
library(tseries)
```

What was a welcome message.

This library is really cool e.g.

```{r}
ibm_stock_history = get.hist.quote(instrument = "IBM", start = "2018-01-01", end = "2018-02-01")
ibm_stock_history
```

Is this a data frame?

```{r}
class(ibm_stock_history)
```

Nope - they made their own data type. If we had a unit on "writing your own R packages", I would explain how this is done but alas there is no time...

Let's say you're sharing your code with someone and one of your lines is loading a library e.g.

```{r}
library(a_library_my_computer_does_not_have_installed_yet)
```

And my computer doesn't have this library. Then we need to stop what we're doing and install. This could be annoying. Here is a convenience: use the pacman package that installs if necessary:

```{r}
if (!require("pacman")){install.packages("pacman")} #installs pacman if necessary but does not load it!
pacman::p_load(devtools) #ensures that devtools gets installed and loaded into the workspace but pacman does not (very tidy)!
```

It is typical to then have a few lines declaring all packages on top of your R/Rmd script file. Here is an example header from one of my projects. 

```{r}
#if (!require("pacman")){install.packages("pacman")}
#pacman::p_load(knitr, randomForest, dplyr, tidyverse, doParallel, xtable, pracma, yaml)
```

We will be seeing this in pretty much all our demos in the future. It is very rare to be coding in R without making use of packages beyond base R. I'm going to require the use of pacman for HW / projects, etc. Just makes code easier to share, run, etc.

The devtools package is important for modern R usage. It allows downloading R packages directly from source that are not even on CRAN. This allows you to get "bleeding edge" features. For example:

```{r}
install_github("yihui/knitr")
```

However this doesn't always work!

```{r}
install_github("hadley/ggplot2")
```

Why can this fail? Because the computer you're running this on is not setup for compiling C++. Admittedly, MAC's usually succeed here and Windows usually fails here. To make it succeed you need to install a separate program beyond R called Rtools. This is one of the big advantages of using Linux and MAC over Windows - Windows just is more buggy when it comes to "real coding" and it gets in the way when you're out there trying to get stuff done. Linux absolutely rules here and because Linux is usually the production environment anyway, it may make sense to use it for all your assignments and coding anyway.

Note, you can use the pacman library for this type of installation too. So your header becomes:

```{r}
if (!require("pacman")){install.packages("pacman")}
pacman::p_load(devtools)
pacman::p_load_gh("hadley/ggplot2")
```

# Convenient Functions for Lists with the purrr package

We first load the library.

```{r}
pacman::p_load(purrr)
```

We will see later that the library `purrr` is part of a collection of libraries called the `tidyverse`.

Now imagine you have a collection of objects in a list. For example, let's let the object be matrices with different sizes:

```{r}
my_matrix_list = list()
my_matrix_list[["first"]] = matrix(rnorm(9), nrow = 3)
my_matrix_list[["second"]] = matrix(rnorm(12), nrow = 2)
my_matrix_list[["third"]] = matrix(rnorm(8), nrow = 4)
my_matrix_list
```

And you want to operate on each of those objects and return a list. Let's say I want to get back the dimensions, or the first rows, or the average values and return the same keys:

```{r}
my_dims_list = modify(my_matrix_list, ~ dim(.x))
my_dims_list
my_first_rows_list = modify(my_matrix_list, ~ .x[1, ])
my_first_rows_list
my_avgs_list = modify(my_matrix_list, ~ mean(.x))
my_avgs_list
```

This is a very convenient function known as "mapping" in functional programming. It saves a few lines of code e.g. the first `modify` would be:

```{r}
my_dims_list = list() #make new list to store keys --> dimensions of original matrices
for (key in names(my_matrix_list)){ #iterate over all list by key
  .x = my_matrix_list[[key]] #get value at the key for this iteration
  my_dims_list[[key]] = dim(.x) #run function on value and save it to new list
}
```

The above which takes 5 lines and is repeated again and again and again in code all takes one line using the `modify` function. 

The `modify` function uses funky syntax which is not standard R. And it doesn't have to be; packages are allowed to extend the language and use symbols to create their own little mini-language. The `.x` above is a dummy variable for the value in the iteration in the imagined for loop (like in my rewritten boilerplate code above). The "~" tilde symbol we will be seeing in base R later on in class but in a completely different context. Here it just means "run the following function".

Modify is just one of the functions in the `purrr` package. See the following cheatsheet for more convenient functions: https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf.


## Loading datasets from R packages

Since R is a language built for data and statistics, it has a ton of interesting data sets by default and even more that are contained in packages. There is really just one command to know:

```{r}
rm(list = ls())
data(iris) #load the iris dataset (as a data frame)
class(iris)
#3 things I always do immediately upon getting a dataset
head(iris)
str(iris)
summary(iris)
```

R has so many datasets. Here they all are by package installed:

```{r}
data(package = .packages(all.available = TRUE))
```

## Continue discussion concerning data frames and the modeling from class

We quickly recreate our data frame from last class:

```{r}
n = 100
X = data.frame(
  salary = round(rnorm(n, 50000, 20000)),
  has_past_unpaid_loan = rbinom(n, size = 1, prob = 0.2),
  past_crime_severity = sample(
    c("no crime", "infraction", "misdimeanor", "felony"),
    size = n,
    replace = TRUE,
    prob = c(.50, .40, .08, .02)
  )
)
row.names(X) = c(
  "Sophia", "Emma", "Olivia", "Ava", "Mia", "Isabella", "Riley", 
  "Aria", "Zoe", "Charlotte", "Lily", "Layla", "Amelia", "Emily", 
  "Madelyn", "Aubrey", "Adalyn", "Madison", "Chloe", "Harper", 
  "Abigail", "Aaliyah", "Avery", "Evelyn", "Kaylee", "Ella", "Ellie", 
  "Scarlett", "Arianna", "Hailey", "Nora", "Addison", "Brooklyn", 
  "Hannah", "Mila", "Leah", "Elizabeth", "Sarah", "Eliana", "Mackenzie", 
  "Peyton", "Maria", "Grace", "Adeline", "Elena", "Anna", "Victoria", 
  "Camilla", "Lillian", "Natalie", "Jackson", "Aiden", "Lucas", 
  "Liam", "Noah", "Ethan", "Mason", "Caden", "Oliver", "Elijah", 
  "Grayson", "Jacob", "Michael", "Benjamin", "Carter", "James", 
  "Jayden", "Logan", "Alexander", "Caleb", "Ryan", "Luke", "Daniel", 
  "Jack", "William", "Owen", "Gabriel", "Matthew", "Connor", "Jayce", 
  "Isaac", "Sebastian", "Henry", "Muhammad", "Cameron", "Wyatt", 
  "Dylan", "Nathan", "Nicholas", "Julian", "Eli", "Levi", "Isaiah", 
  "Landon", "David", "Christian", "Andrew", "Brayden", "John", 
  "Lincoln"
)
X
```

Remember our cross tab? Now we can get fancier using our new libary skills. Any Stata fans out there?

```{r}
pacman::p_load(gmodels)
CrossTable(X$has_past_unpaid_loan, X$past_crime_severity, chisq = TRUE)
```


And add a new variable, the response to the data frame:

```{r}
X$paid_back_loan = factor(rbinom(n, size = 1, prob = 0.9), labels = c("No", "Yes"))
```

Note that our matrix is now no longer just $X$; it includes $y$. I could make a renamed copy, but I want to show off dropping this column and create a new object that's both features and response column-binded together:

```{r}
y = X$paid_back_loan
X$paid_back_loan = NULL #drop column
Xy = cbind(X, y) #an aside: what do you think the "rbind" function does?
head(Xy) #make sure that worked
summary(Xy) #much better now!
#Note: Xy = X; rm(X) would've been easier
```

I prefer calling the full training set ${X, y}$ a data frame called $Xy$. 

Most data sets are names some descriptive name like "loandata" or "cars". Here are undoubtedly some of the most famous datasets:

```{r}
data(Boston, package = "MASS") #package argument not needed if package loaded 
head(Boston)
data(iris)
head(iris)
```


The object $X$ is now extraneous, so we should clean up our workspace now.

```{r}
rm(list = setdiff(ls(), "Xy"))
```


## The Null Model

```{r}
g = function(x){
  #return mode regardless of x
} 

g(5000)
```


## The Threshold Model

Let's compute the threshold model and see what happens. Here's an inefficent but quite pedagogical way to do this:

```{r}
n = nrow(Xy)
num_errors_by_parameter = matrix(NA, nrow = n, ncol = 2)
colnames(num_errors_by_parameter) = c("threshold_param", "num_errors")
y_logical = Xy$y == "Yes"
for (i in 1 : n){
  threshold = Xy$salary[i]
  num_errors = sum((Xy$salary > threshold) != y_logical)
  num_errors_by_parameter[i, ] = c(threshold, num_errors)
}
num_errors_by_parameter

#now grab the smallest num errors
best_row = order(num_errors_by_parameter[, "num_errors"])[1]
x_star = c(num_errors_by_parameter[best_row, "threshold_param"], use.names = FALSE)
x_star
```

Let's program `g`, the model that is shipped as the prediction function for future `x_*`

```{r}
g = function(x){
  ifelse(x > x_star, 1, 0)
} 

g(5000)
```


## The Perceptron

Time for some new data first... we are bored of the fabricated creditworthiness data.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 10] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

First question. Let $\mathcal{H}$ be the set $\{0, 1\}$ meaning $g = 0$ or $g = 1$. What are the error rates then on $\mathbb{D}$? 

```{r}
#If always 0, all the 1's are errors
239 / (444 + 239)
#If always 1, all the 0's are errors
444 / (444 + 239)
```

If your $g$ can't beat that, either your features $x_1, \ldots, x_p$ are terrible, and/or $\mathcal{H}$ was a terrible choice and/or $\mathcal{A}$ can't pull its weight.

Okay... back to the "perceptron learning algorithm".

Let's do so for one dimension - just "V1" in the breast cancer data. You will do an example with more features for the lab.

```{r}
y_binary = ifelse(y_binary == 1, 0, 1)
MAX_ITER = 1000
w_vec = rep(0, 2) #intialize a 2-dim vector

X1 = as.matrix(cbind(1, X[, 1, drop = FALSE]))

for (iter in 1 : MAX_ITER){  
  for (i in 1 : nrow(X1)){
    x_i = X1[i, ]
    yhat_i = ifelse(sum(x_i * w_vec) > 0, 1, 0)
    y_i = y_binary[i]
    w_vec[1] = w_vec[1] + (y_i - yhat_i) * x_i[1]
    w_vec[2] = w_vec[2] + (y_i - yhat_i) * x_i[2]
  }
}
w_vec
```

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Looks like the perceptron fit to just the first feature beat the null model (at least on the data in $\mathbb{D}$). Is this expected? Yes if the first feature is at all predictive of `y`.


## Matrix operations in R

R can do all the standard matrix operations. Let's go through them quickly. First initialize two example matrices:

```{r}
A = matrix(rep(1, 4), nrow = 2)
A
B = array(seq(1, 4), dim = c(2, 2))
B
I = diag(2) #create an identity matrix of size 2x2
I
```

Now we show off some operations:

```{r}
A * B #element-wise multiplication
A %*% B #matrix multiplication
B %*% I
t(B) #transpose
solve(B)
solve(A) #BOOM - why?
solve(I)
#rank(A) = 1 #no such function... but... there are tons of add-on libraries for matrix computations e.g.
pacman::p_load(Matrix) #load the Matrix library
rankMatrix(B)
rankMatrix(A)
rankMatrix(I)
```

Note that vectors and matrices are not the same:

```{r}
v = c(1, 2, 3) #3-d vector
t(v) #converts to 1x3 vector... unsure why
t(t(v))
v %*% v #seems to default to dot product
t(v) %*% t(t(v)) #dot product
I = diag(3)
I %*% v #seems to default correctly!
I %*% t(v) #actually uncomformable
```

## The linear threshold model

Using the algorithm $\mathcal{A}$ discussed previously, this model was dubbed the "perceptron". However, there are other algorithms $\mathcal{A}$ we can use to fit the "best" model in $\mathcal{H}$.

We spoke about how the `w` vector is in a large space, $R^p$. A course in optimization will describe methods how to find optimal and approximately optimal solutions for `w` based on an "objective function" or "fitness function" or "cost function". Here that target is the number of total errors of the `n` examples in the training data set.

```{r}
SAE = function(w_vec){
  yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
  sum(y_binary != yhat)
}
```


Here are some off-the-shelf solvers in R in action. Note: this is not the classic "perceptron learning algorithm". Also: this is complete junk made-up data so it's only an illustration. You will see real data for your homework.

Let's an optimization algorithm called Nelder-Mead developed by John Nelder and Roger Mead (both were famous statisticians) in 1965.

```{r}
pacman::p_load(optimx)
?optimx
optim_output = optimx(c(50, -10), SAE, method = "Nelder-Mead")
optim_output
w_vec = t(as.matrix(optim_output[1:2]))

```

What is our error rate using the Nelder-Mead local optimum?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Not good. Let's try again with the perceptron input as a starting point.

Lesson: perceptron learning algorithm maybe not so great. Also - different $\mathcal{A}$'s give you wildly different models $g$ even with the same allowable functions provided in $\mathcal{H}$. 

Most of the really creative work in modeling is within $\mathcal{A}$ although specifying $\mathcal{H}$ suitably flexible is very important too.

## Nearest Neighbor algorithm

Load up the breast cancer data set again.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data with all observations with missing values dropped
X = Xy[, 2 : 10] #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")
```

In one dimension, we are looking for the closest x. Here is a simple function that does it:

```{r}
nn_function = function(x_star){
  best_sqd_distance = Inf #good place to begin
  i_star = NA
  for (i in 1 : nrow(X)){
    dsqd = (X[i, 1] - x_star)^2
    if (dsqd < best_sqd_distance){
      best_sqd_distance = dsqd
      i_star = i
    }
  }
  y_binary[i_star]
}
nn_function(7.8)
nn_function(5.2)
```

Why is this silly for this dataset?

```{r}
str(X)
```

The features are not truly continuous. Would it make sense in higher dimensions? Your homework...

Has this been coded before? Definitely...

```{r}
pacman::p_load(class)
?knn
```

We can fit a knn model *and* predict in one shot via:

```{r}
y_hat = knn(X, c(4, 2, 1, 1, 2, 1, 2, 1, 1), y_binary, k = 1)
y_hat
```

Why is build model and predict in one shot natural in knn?

Now for an interesting exercise that will setup future classes:

```{r}
y_hat = knn(X, X, y_binary, k = 1)
y_hat
all.equal(y_hat, factor(y_binary))
```

No errors! Can this be a good model? No... "something" must be wrong! It is too good to be true.

Something is wrong. This is the first example of "overfitting". We will explore this in depth.

```{r}
rm(list = ls())
```


## Support Vector Machines (SVM)

You will code a basic SVM for homework. Here we use the `e1071` library.

```{r}
pacman::p_load(e1071)
```

We make a simple dataset first.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```

And let's plot the data:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Now we fit a linear SVM. Since it is linearly separable, we can make $\lambda = 0$. Since the package doesn't allow zero, we make it trivially small. 

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
lambda = 1e-9
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
```

Now we calculate the weight vector. This is technical and not covered in the class yet (or maybe never):

```{r}
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
w_vec_simple_svm
```

and plot it:

```{r}
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

To understand the hyperparameter, let's introduce another data point so the training data is no longer linearly separable.

```{r}
Xy_simple = rbind(Xy_simple, c(0, 3.2, 3.2))
```

and plot it:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Let's try SVM at different $\lambda$ values.

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
lambda = 1.3
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)

simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

What lesson did we learn here? This hyperparameter really matters! We need to figure out a way to deal with this later. So far neither the perceptron nor the SVM is an algorithm without flaws.


## Errors and Warnings

You can write better functions if you make use of errors and warnings. Java forces you to catch likely errors via the "throws" designation for a method but there is no such requirement in R.

* Errors are unrecoverable, they halt execution i.e. red lights
* Warnings (under usual execution) do not halt execution, but they display a message, i.e. yellow lights

Here's how they work:

```{r}
my_vector_sum = function(xs){
  
  if (!(class(xs) %in% c("numeric", "integer"))){ #short for class(xs) == "numeric" | class(xs) == "integer"
    stop("You need to pass in a vector of numbers not a vector of type \"", class(xs), "\".\n") #throw error!
    #warning("Your vector of type \"", class(xs), "\" will be coerced to numbers.\n") #throw warning!
    xs = as.numeric(as.factor(xs))
  }
  
  tot = 0
  for (x in xs){
    tot = tot + x
  }
  tot
}
my_vector_sum(c(1, 2, 3))
my_vector_sum(c("a", "b", "c"))
```

There is a try-catch as well:

```{r}
xs = c("a", "b", "c")
tot = my_vector_sum(xs)

tot = tryCatch(
  {
    my_vector_sum(xs)
  },
  error = function(e){
    print("xs was non-numeric... coercing xs to numeric...")
    my_vector_sum(as.numeric(as.factor(xs)))
  }
)
tot
```

The recommended thing to do of course is to query if it is non-numeric within the function `my_vector_sum` and cast it then. Possibly create an argument toggling this behavior on/off with a default of off.