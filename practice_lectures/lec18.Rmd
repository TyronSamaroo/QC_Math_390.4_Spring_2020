---
title: "lec1.Rmd"
author: "Adam Kapelner"
date: "May 5, 2020"
output: html_document
---

First load the tree package:

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF, tidyverse, magrittr, data.table)
```

# Validation in Bagging?

We are using the "bootstrap" to get the trees. Can we do model validation in the same step? 

The answer is yes. For every tree, there was a bootstrap sample of the training set used to build the tree. But there are observations in $\mathbb{D}$ that are not in the bootstrap sample! About 1/3 on average are left out i.e. "out of bag (oob)". Over many trees, there are different oob subsets than become the full data set. So you actually have validation in a way on the whole dataset kind of like K-fold cross validation. Supposedly this validation is similar to K=2 in terms of performance. It is what everyone seems to use. 

Let's load the data and packages from last class and plot the data:

```{r}
rm(list = ls())
n_train = 100
n_test = 500
xmin = 0
xmax = 10
sigma = 0.09
num_trees = 500
x_train = runif(n_train, xmin, xmax)
delta_train = rnorm(n_train, 0, sigma)
y_train = sin(x_train) + delta_train
ggplot(data.frame(x = x_train, y = y_train)) + geom_point(aes(x, y))
```

Let's look at one bagged tree model and compute OOB errors after construction. Here we just drop the `calculate_oob_error` argument as the default is `TRUE`. We save the model and print it out:

```{r}
bagged_tree_mod = YARFBAG(data.frame(x = x_train), y_train, num_trees = num_trees)
bagged_tree_mod
```

How did this work? Let's look at the oob sets of indices:

```{r}
cat("bootstrap indices:")
sort(bagged_tree_mod$bootstrap_indices[[1]])
cat("oob:")
sort(setdiff(1 : n_train, bagged_tree_mod$bootstrap_indices[[1]]))
cat("bootstrap indices:")
bagged_tree_mod$bootstrap_indices[[2]]
cat("oob:")
setdiff(1 : n_train, bagged_tree_mod$bootstrap_indices[[2]])
bagged_tree_mod$y_oob

n_oob = sapply(1 : n_train, function(i){sum(unlist(lapply(bagged_tree_mod$bootstrap_indices, function(set){!(i %in% set)})))})
round(n_oob / num_trees, 2)
```

It took predictions on each tree on the oob set, averaged by observation across trees and then averaged across observation averages.

Let's compare this to training-test manual splits. Let's look at the boston housing data first.

```{r}
boston = MASS::Boston %>% data.table

seed = 1
set.seed(seed)
prop_test = 0.2
test_indices = sample(1 : nrow(boston), round(prop_test * nrow(boston)))
boston_test = boston[test_indices, ]
y_test = boston_test$medv
X_test = boston_test
X_test$medv = NULL
train_indices = setdiff(1 : nrow(boston), test_indices)
boston_train = boston[train_indices, ]
y_train = boston_train$medv
X_train = boston_train
X_train$medv = NULL

num_trees = 500

#build on training and validate on test
mod_bag_train = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE, seed = seed)
y_hat_test_bag = predict(mod_bag_train, X_test)
s_e_bag = sd(y_test - y_hat_test_bag)
s_e_bag

#build and validate on all the data at once!
mod_bag_all = YARFBAG(boston[, !"medv"], boston$medv, num_trees = num_trees, seed = seed)
mod_bag_all$rmse_oob
```

There is a lot of variation, but theoretically, they should be about the same. 

# Random Forests


What do we have now? We have model selection is done within training. And training and validation are done in a single step! No more costly K-fold CV with 3 splits!

But can it get any better? YES. As you saw, the variance terms can be shrunk further the more decorrelated the trees become. We do this now by introducing randomness into the splits by choosing only a subset of the features to split on randomly. The trees are then grown as normal. Then the we model average many trees via bagging. And that's random forests!

Quick demo with the boston housing data and oob validation:

```{r}
rm(list = ls())
boston = MASS::Boston
y = boston$medv
X = boston
X$medv = NULL

seed = 1
num_trees = 500
mod_bag = YARFBAG(X, y, num_trees = num_trees, seed = seed)
mod_bag
mod_rf = YARF(X, y, num_trees = num_trees, seed = seed)
mod_rf
```

Gain in decorrelation?

```{r}
cat("gain: ", (mod_bag$pseudo_rsq_oob - mod_rf$pseudo_rsq_oob) / mod_bag$pseudo_rsq_oob * 100, "%\n")
```

For this example, not much. How about on the diamonds dataset? Here the dataset is too big to fit a regression tree. So we'll subsample train and we can subsample test because it's free.

```{r}
diamonds = ggplot2::diamonds

rm(list = ls())
num_trees = 500
seed = 1

n_train = 500

training_indices = sample(1 : nrow(diamonds), n_train)
diamonds_train = diamonds[training_indices, ]
y_train = diamonds_train$price
X_train = diamonds_train
X_train$price = NULL

mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE, seed = seed)
mod_rf = YARF(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE, seed = seed)

test_indices = setdiff(1 : nrow(diamonds), training_indices)
diamonds_test = diamonds[test_indices, ]
y_test = diamonds_test$price
X_test = diamonds_test
X_test$price = NULL

mse_bag = sum((y_test = predict(mod_bag, X_test))^2) / nrow(X_test)
mse_rf = sum((y_test = predict(mod_rf, X_test))^2) / nrow(X_test)
cat("gain: ", (mse_bag - mse_rf) / mse_bag * 100, "%\n")
```

Also, not much, but real.


#Classification Trees and Confusion Tables

Let's load up the adult dataset where the response is 1 if the person makes more than $50K per year and 0 if they make less than $50K per year.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult %<>% 
  na.omit #kill any observations with missingness
str(adult)
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
test_size = 2000
train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Load the tree package:

```{r}
options(java.parameters = "-Xmx4000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
pacman::p_load(YARF)
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train)
```

How "big" is this tree model?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

Compute in-sample and out of sample fits:

```{r}
y_hat_train = predict(tree_mod, X_train)
y_hat_test = predict(tree_mod, X_test)
```

Let's look at the confusion table in-sample:

```{r}
table(y_train, y_hat_train)
```

There are no errors here! Thus, precision and recall are both 100%. This makes sense because classification trees overfit.

Let's do the same oos:

```{r}
oos_conf_table = table(y_test, y_hat_test)
oos_conf_table
```

We didn't do as well (of course). Let's calculate some performance metrics. We assume ">50k" is the "positive" category and "<=50k" is the "negative" category. Note that this choice is arbitrary and everything would just be switched if we did it the other way.

```{r}
n = sum(oos_conf_table)
n
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
acc = (tp + tn) / n
acc
misclassifcation_error = 1 - acc
misclassifcation_error
precision = tp / num_pred_pos
precision
recall = tp / num_pos
recall
false_discovery_rate = 1 - precision
false_discovery_rate
false_omission_rate = fn / num_pred_neg
false_omission_rate
```

Let's see how this works on a dataset whose goal is classification for more than 2 levels. Note: this is only possible now with trees!

```{r}
rm(list = ls())
pacman::p_load(mlbench)
data(LetterRecognition)
LetterRecognition = na.omit(LetterRecognition) #kill any observations with missingness
str(LetterRecognition)
?LetterRecognition
```

Now we split the data:

```{r}
test_samp = 2000
train_indices = sample(1 : nrow(LetterRecognition), test_samp)
ltr_train = LetterRecognition[train_indices, ]
y_train = ltr_train$lettr
X_train = ltr_train
X_train$lettr = NULL
test_indices = sample(setdiff(1 : nrow(LetterRecognition), train_indices), test_samp)
ltr_test = LetterRecognition[test_indices, ]
y_test = ltr_test$lettr
X_test = ltr_test
X_test$lettr = NULL
```

And fit a tree model and its in-sample and oos fits:

```{r}
tree_mod = YARFCART(X_train, y_train)
y_hat_train = predict(tree_mod, X_train)
y_hat_test = predict(tree_mod, X_test)
```

Take a look at the in-sample confusion matrix:

```{r}
table(y_train, y_hat_train)
```

Perfecto... as expected... 

Now the oos confusion matrix:

```{r}
oos_confusion_table = table(y_test, y_hat_test)
oos_confusion_table
```

Hard to read. Let's make it easier to read by blanking out the diagonal and looking at entried only >= 5:

```{r}
oos_confusion_table[oos_confusion_table < 5] = ""
diag(oos_confusion_table) = "."
oos_confusion_table
```

What's it using to determine letter?

```{r}
illustrate_trees(tree_mod, max_depth = 3, open_file = TRUE)
```

Where did these features comes from?? Deep learning helps to create the features from the raw pixel data. Wish I had a whole next semester to discuss this...

Random Forests:

```{r}
num_trees = 500
n_train = 500

training_indices = sample(1 : nrow(adult), n_train)
adult_train = adult[training_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_rf = YARF(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
```

And test:

```{r}
test_indices = sample(setdiff(1 : nrow(adult), training_indices), 25000)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL

y_hat_test_bag = predict(mod_bag, X_test)
y_hat_test_rf = predict(mod_rf, X_test)

oos_conf_table_bag = table(y_test, y_hat_test_bag)
oos_conf_table_rf = table(y_test, y_hat_test_rf)
oos_conf_table_bag
oos_conf_table_rf
miscl_err_bag = mean(y_test != y_hat_test_bag)
miscl_err_rf = mean(y_test != y_hat_test_rf)
miscl_err_bag
miscl_err_rf

cat("gain: ", (miscl_err_bag - miscl_err_rf) / miscl_err_bag * 100, "%\n")
```

And on letters:

```{r}
test_samp = 2000
train_indices = sample(1 : nrow(LetterRecognition), test_samp)
ltr_train = LetterRecognition[train_indices, ]
y_train = ltr_train$lettr
X_train = ltr_train
X_train$lettr = NULL
test_indices = sample(setdiff(1 : nrow(LetterRecognition), train_indices), test_samp)
ltr_test = LetterRecognition[test_indices, ]
y_test = ltr_test$lettr
X_test = ltr_test
X_test$lettr = NULL
```

And fit a tree model and its in-sample and oos fits:

```{r}
mod_bag = YARFBAG(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_rf = YARF(X_train, y_train, num_trees = num_trees, calculate_oob_error = FALSE)
mod_bag
mod_rf

oos_conf_table_bag = table(y_test, y_hat_test_bag)
oos_conf_table_rf = table(y_test, y_hat_test_rf)
oos_conf_table_bag
oos_conf_table_rf
miscl_err_bag = mean(y_test != y_hat_test_bag)
miscl_err_rf = mean(y_test != y_hat_test_rf)
miscl_err_bag
miscl_err_rf

cat("gain: ", (miscl_err_bag - miscl_err_rf) / miscl_err_bag * 100, "%\n")
```

Very real gains for classification.


# Correlation does not Imply Causation

Take a look at the following real data:

```{r}
spurious = data.frame(
  yearly_divorce_rate_maine_per_1000 = c(5,4.7,4.6,4.4,4.3,4.1,4.2,4.2,4.2,4.1),
  yearly_US_consumption_margarine_per_capita = c(8.2,7,6.5,5.3,5.2,4,4.6,4.5,4.2,3.7)
)

ggplot(spurious, aes(x = yearly_divorce_rate_maine_per_1000, y = yearly_US_consumption_margarine_per_capita)) +
  geom_point() + geom_smooth()
```

How did this happen? 

Well, we can imagine doing the same thing.


```{r}
y = spurious$yearly_US_consumption_margarine_per_capita
n = length(y)

r = 100000
best_abs_corr = 0
best_random_xs = NULL
for (i in 1 : r){
  x = rnorm(n)
  
  random_abs_corr = abs(cor(x, y))
  if (random_abs_corr > best_abs_corr){
    best_abs_corr = random_abs_corr
    best_random_xs = x
  }
}
spurious$best_random_xs = best_random_xs
ggplot(spurious, aes(x = best_random_xs, y = yearly_US_consumption_margarine_per_capita)) +
  geom_point() + geom_smooth() + ggtitle(paste("Spurious Correlation has |r| = ", round(best_abs_corr, 3)))
```

So what's the narrative here? If you look through a gajillion random features that have no causal connection with the phenomenon $y$, you will eventually find something that "clicks". However, this will be vanish if you keep collecting data. Anything that is built upon falsehood will crumble!

#An Example of Correlation without Causation

When does correlation really not imply causation? From class, we spoke about the phenomenon $y$ = "num car accidents" with observed feature "x" = "num umbrellas sold" but common cause $z$ = "rain amount". It is clear the umbrella sales has *no causal* relationship with car accidents. But they *are correlated* because they are linked by a common cause. Here's the data example that makes this clear.

The data generating process as specified by the causal diagram looks as follows:

```{r}
rm(list = ls())
set.seed(1)
n = 300
sigma = 0.3

umbrella_example_data = data.frame(
  z_rainfall = runif(n, 0, 6) #here's the common cause - rainfall
)
umbrella_example_data$x_umbrella_sales = umbrella_example_data$z_rainfall^2 + rnorm(n, sigma) #x is a variable that is driven by z with noise
umbrella_example_data$y_car_accidents = umbrella_example_data$z_rainfall + rnorm(n, sigma) #y is a variable driven by z with noise
```

So we only see $x$ and $y$. Here's what it appears as:

```{r}
pacman::p_load(ggplot2)
ggplot(umbrella_example_data) +
  aes(x = x_umbrella_sales, y = y_car_accidents) +
  geom_point() + 
  geom_smooth(method = "lm")
```

and the model looks like:

```{r}
mod = lm(y_car_accidents ~ x_umbrella_sales, umbrella_example_data)
round(coef(mod), 3)
```

So what's the interpretation of the coefficient for $x$? ...

What you can't say is that $x$ is a causal contributor to $y$! You may want to say it, but you can't!

Now let's build a model of $y$ linear in both $x$ and $z$. What happens?

```{r}
mod = lm(y_car_accidents ~ x_umbrella_sales + z_rainfall, umbrella_example_data)
round(coef(mod), 3)
```

The effect of $x$ is gone!! Why? If you keep $z$ constant, the sole true causal factor in $y$, manipulating $x$ won't matter anymore!

Why is this? Well, you can look at how x affects y in local areas of z for instance.

```{r}
b = 0.8; a = 0.7
z_small_indices = umbrella_example_data$z_rainfall < 
  quantile(umbrella_example_data$z_rainfall, b) &
  umbrella_example_data$z_rainfall >
  quantile(umbrella_example_data$z_rainfall, a)

ggplot(umbrella_example_data[z_small_indices, ]) +
  aes(x = x_umbrella_sales, y = y_car_accidents) +
  geom_point()
```

If you force the common cause (lurking variable) to be an approximate constant, then you won't see any affect of x on y.



# Missingness

Take a look at the weather dataset again:

```{r}
pacman::p_load(nycflights13, tidyverse, magrittr)
data(weather)
str(weather)
summary(weather)
```

Note how `time_hour` is perfectly collinear with the year, month, day and hour columns. So let's drop `time_hour`:

```{r}
weather %<>%
  select(-c(time_hour))
```

We also cannot have character variables:

```{r}
weather %<>%
  mutate(origin = as.factor(origin))
```


Imagine we were trying to predict `precip`. So let's section our dataset:

```{r}
y = weather$precip
X = weather
X$precip = NULL
rm(weather)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = tbl_df(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
head(M)
summary(M)
```

Some of these missing indicators are collinear because they share all the rows they are missing on. Let's filter those out:

```{r}
M = tbl_df(t(unique(t(M))))
```


Many featuers did not have missingness so let's remove them:

```{r}
M %<>% select_if(function(x){sum(x) > 0})
head(M)
dim(M)
colSums(M)
```

Now let's work on imputation. The missingness is not extreme except in the `wind_gust` variable. Let's drop that variable:

```{r}
X %<>% select(-wind_gust)
```

Without imputing and without using missingness as a predictor in its own right, let's see what we get with a basic linear model now:

```{r}
lin_mod_listwise_deletion = lm(y ~ ., X)
summary(lin_mod_listwise_deletion)
```

A measly 13.2%. Also note: year is collinear.

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 26,000! observations) so we will sample 2,000 for each of the trees and then average. That will be good enough.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(2000, ncol(X)))$ximp
```

Now we take our imputed dataset, combine it with our missingness indicators for a new design matrix.

```{r}
Xnew = data.frame(cbind(Ximp, M))
linear_mod_impute_and_missing_dummies = lm(y ~ ., Xnew)
summary(linear_mod_impute_and_missing_dummies)
```

Is this a better model?? Are they even comparable? How to compare them? I'm not sure... certainly an R^2 comparison is the wrong way to compare.

Note: this is just an illustration of best practice. It didn't necessarily have to "work".

It is hard to compare the two models since the first model was built with 23,000 observations and this was built with the full 26,000 observations. Those extra 3,000 are the most difficult to predict on. This is complicated...

