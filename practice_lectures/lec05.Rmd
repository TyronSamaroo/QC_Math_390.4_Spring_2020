---
title: "Practice Lecture 5 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 26, 2019"
---



## Multivariate linear regression

We want to run a multivariate linear regression $\mathcal{H}$ employing the least squares $\mathcal{A}$ manually using our derived linear algebra. Let us first pull out $\mathbb{D}$ as $y$ and $X$.

Let's ensure we augment the `X` to include the 1 vector in front. We need this for the intercept in the $w$ vector in our spec, $\mathcal{H}$.

```{r}
y = boston$medv
X = cbind(1, boston[, 1: 13])
```

Can we find $X^\top X$?

```{r}
# XtX = t(X) %*% X
```

The data frame is great, but unfortunately R does not allow us to use matrix algebra on it.

So let's create a matrix. Note: there are no factor variables with more than one level. `chas` is a binary variable and that's okay. If there were factors with more than level, the following will not work. We will explore this later.

```{r}
X = as.matrix(cbind(1, boston[, 1: 13]))
```

So $p = 12$ and $p + 1 = 14$.

Let's make each predictor name nice just for aesthetic value:

```{r}
colnames(X)
colnames(X)[1] = "(intercept)" #this is the standard way lm denotes it (which we will compare to later)
colnames(X)
```


Can we find $X^\top X$?

```{r}
XtX = t(X) %*% X
```

Is it full rank?

```{r}
XtXinv = solve(XtX)
```

It worked. This means $X$ is full rank i.e. there is no linear duplication of information over the `13 + 1` predictors. In case we're in doubt:

```{r}
pacman::p_load(Matrix)
rankMatrix(X)[[1]]
rankMatrix(t(X))[[1]]
rankMatrix(XtX)[[1]]
rankMatrix(XtXinv)[[1]]
```


Let's calculate the LS solution then:

```{r}
b = XtXinv %*% t(X) %*% y
b
```

Interpretation: if `crim` "increases" by 1, $\hat{y}$ increases by... etc etc. How would `crim` increase? Big philosophical topic which we are punting on (for now). If all predictors are 0, then $y$ would be predicted to be the intercept, 20.65. Strange concept... not usually important.

What would $g$ look like?

```{r}
g_predict_function = function(x_star){
   x_star %*% b
}
g_predict_function(X[7, ])
y[7] #good prediction!
```

Pretty simple...  and `x_star` could be a matrix of `n_star * (p + 1)` - where `n_star` is however many new observations you wish to predict.

We can compute all predictions:

```{r}
yhat = X %*% b
```

Can you tell this is projected onto a 13 dimensionsal space from a 506 dimensional space? Not really... but it is...

We can calculate the residuals:

```{r}
e = y - yhat
head(e)
```

What is RMSE?

```{r}
SSE = t(e) %*% e
MSE = 1 / (ncol(X)) * SSE
RMSE = sqrt(MSE)
SSE
MSE
RMSE
```

Interpret the RMSE...

We can calculate $R^2$ two ways:

```{r}
s_sq_y = var(y)
s_sq_e = var(e)
Rsq = (s_sq_y - s_sq_e) / s_sq_y
Rsq

n = length(e)
SST = (n - 1) * s_sq_y
Rsq = 1 - SSE / SST
Rsq
```

Let's look at distribution of $y$ and $e$ to get an idea about $R^2$ as we did before:


```{r}
pacman::p_load(ggplot2)

ggplot(data.frame(null_residuals = y - mean(y), residuals = e)) + 
  stat_density(aes(x = residuals), fill = "darkgreen", alpha = 0.3) + 
  stat_density(aes(x = null_residuals, fill = "red", alpha = 0.3)) +
  theme(legend.position = "none")
```

What does this tell you about $R^2$?

Now, of course, R has its own function to do all this. We've already seen them! To run a multivariate least squares linear model,

```{r}
mult_lin_mod = lm(medv ~ ., boston)
```

No need to (a) create a matrix from the data frame (b) append a 1's column (c) do the linear algebra. It's all done for you. What is this formula `medv ~ .`? Previously we've seen `medv ~ rm` to indicate "fit phenomenon `medv` using predictor `rm`". Here, it's "fit phenomenon `medv` using all available predictors in the data frame". This is a very powerful formula!

Let's extract the estimates $b$ as well as $R^2$ and RMSE:

```{r}
coef(mult_lin_mod)
summary(mult_lin_mod)$r.squared
summary(mult_lin_mod)$sigma
```

Does R offer a simple way to do $g$? Sure...

```{r}
x_star = boston[7, ]
y_hat_star = predict(mult_lin_mod, newdata = x_star)
y_hat_star
y[7]
```

If you care about the internals of what R is doing, it retraces our steps perfectly. It first creates the "model matrix" we called the "design matrix" and denoted it X:

```{r}
Xmm = model.matrix(medv ~ ., boston)
head(Xmm)
head(X) #same
```

Then it uses an internal function to compute the linear algebra:

```{r}
raw_mod = lm.fit(Xmm, y)
raw_mod$coefficients
```

We will soon see the internals of the `lm.fit` algorithm when we get to orthogonal projections in class (coming up next).


